{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Chapter 4: Classical Statistical Inference"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#take care of some things\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.2: Maximum likelihood esitimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section is mostly for me to review the first part of this chapter!  No need to go through this if you have section 4.2 down!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a known model, we can predict the probability of observing any particular value, i.e. the likelihood.  We can write this formally as the likelihood of measuring data, $\\{ x_i \\}$, given a model, $M$, characterized by parameter, $\\theta$, as:\n",
      "\n",
      "\\begin{equation}\n",
      "L\\equiv p(\\{ x_i \\}|M(\\theta)) = \\prod_{i=1}^n p(x_i|M(\\theta))\n",
      "\\end{equation}\n",
      "\n",
      "So, let me try and calculate some likelihoods for a normal distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rv = stats.norm()\n",
      "p = rv.pdf #normal pdf\n",
      "\n",
      "#take a look at the probability distribution\n",
      "x = np.linspace(-10,10,200) #sample x from in the range (-10,10)\n",
      "plt.figure()\n",
      "plt.plot(x,p(x))\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('p(x)')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rv = stats.norm(0)\n",
      "p = rv.pdf #normal pdf\n",
      "#now, lets make some fake data that follows the distribution\n",
      "x = rv.rvs(1000) #draw 100 points from this distribution\n",
      "\n",
      "#now we can calculate the likelihood of drawing just the first point\n",
      "N_draws=1\n",
      "L = np.prod(p(x[0:N_draws]))\n",
      "print 'L(1)=',L\n",
      "\n",
      "#now the first 10 points:\n",
      "N_draws=10\n",
      "L = np.prod(p(x[0:N_draws]))\n",
      "print 'L(10)=',L\n",
      "\n",
      "#and all 100 points:\n",
      "N_draws=100\n",
      "L = np.prod(p(x[0:N_draws]))\n",
      "print 'L(100) = ',L\n",
      "\n",
      "#and all 1000 points:\n",
      "N_draws=1000\n",
      "L = np.prod(p(x[0:N_draws]))\n",
      "Ls = np.sum(np.log(p(x[0:N_draws])))\n",
      "print 'L(1000) = ',L\n",
      "print 'ln(L(1000)) = ',Ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, great, clearly this number is going to start to get very small for large data sets!  Let me see what happens if I pick the wrong model, for this example, a normal distribution centered around 10 instead of 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rv_model = stats.norm(10) #my model is centered on 10\n",
      "p_model = rv_model.pdf #model pdf\n",
      "\n",
      "#now, lets make some fake data that follows the distribution\n",
      "rv_truth = stats.norm(0) #true distribution is centered on 0\n",
      "x = rv_truth.rvs(1000) #draw 1000 points from this distribution\n",
      "\n",
      "#now we can calculate the likelihood of drawing just the first point\n",
      "N_draws=1\n",
      "L = np.prod(p_model(x[0:N_draws]))\n",
      "print 'L(1) =',L\n",
      "\n",
      "#now the first 10 points:\n",
      "N_draws=10\n",
      "L = np.prod(p_model(x[0:N_draws]))\n",
      "print 'L(10) =',L\n",
      "\n",
      "#and all 100 points:\n",
      "N_draws=100\n",
      "L = np.prod(p_model(x[0:N_draws]))\n",
      "print 'L(100) =',L\n",
      "\n",
      "#and all 1000 points:\n",
      "N_draws=1000\n",
      "L = np.prod(p_model(x[0:N_draws]))\n",
      "print 'L(1000) =',L"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great!  Ok, the likeihoods I got were smaller numbers than the ones I got when I jused the correct model!  I can see how using the natural log could help for large data sets."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "sidetrack: $\\chi^2$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I don't remember if we talked about this already, but I needed a review, so here it is.  \n",
      "\n",
      "Consider some data, $\\{x_i\\}$, that is drawn from an intrinisic gaussian distribution with mean, $\\mu$, and standard deviation, $\\sigma$.  we can define:\n",
      "\n",
      "\\begin{equation}\n",
      "z_i=(x_i-\\mu)/\\sigma\n",
      "\\end{equation}\n",
      "\n",
      "The sum of the squares is defined as:\n",
      "\n",
      "\\begin{equation}\n",
      "Q=\\sum_{i=1}^{N}z_i^2\n",
      "\\end{equation}\n",
      "\n",
      "Now, it turns out that this distribution follows a $\\chi^2$ distribution.  This is defined as:\n",
      "\n",
      "\\begin{equation}\n",
      "p(Q|k)\\equiv\\chi^2(Q|k)=\\frac{1}{2^{k/2}\\Gamma(k/2)}\\exp(-Q/2)\n",
      "\\end{equation}\n",
      "\n",
      "Where $k=N$.\n",
      "\n",
      "So, let me check this out."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets define a data set drawn from a normal gaussian\n",
      "N_data=5\n",
      "#np.random.seed(12)\n",
      "rv = stats.norm(0)\n",
      "p = rv.pdf #normal pdf\n",
      "#now, lets make some fake data that follows the distribution\n",
      "x = rv.rvs(N_data) #draw 100 points from this distribution\n",
      "mu = np.mean(x)\n",
      "sigma = 1 #fix one\n",
      "print mu, sigma\n",
      "z = (x - mu) / sigma\n",
      "Q = np.sum(z**2.0)\n",
      "print Q\n",
      "\n",
      "Q=np.zeros(1000)\n",
      "for seed in range(0,1000):\n",
      "    x = rv.rvs(N_data) #draw 100 points from this distribution\n",
      "    mu = np.mean(x)\n",
      "    z = (x - mu) / sigma\n",
      "    Q[seed] = np.sum(z**2.0)\n",
      "\n",
      "bins=np.arange(0,25,0.5)\n",
      "result = np.histogram(Q,bins=bins)[0]\n",
      "result = result/1000.0\n",
      "\n",
      "rv = stats.chi2(5-1)\n",
      "x = rv.rvs(100000)\n",
      "result2 = np.histogram(x,bins=bins)[0]\n",
      "result2 = result2/100000.0\n",
      "\n",
      "plt.figure()\n",
      "plt.bar(bins[:-1],result,width=0.5) #Qs\n",
      "plt.plot(bins[:-1],result2, color='red', lw=5) #chi^2 dist\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.3: Goodness of fit and model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Maximum likelyhood estimators(MLE) give us a best fit parameter and its uncertainities for a model, but how do we compare two models and decide which one is  the best? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's stick to guassian distributions for now.  Remmeber that the likelyhood for a gaussian is:\n",
      "\n",
      "\\begin{equation}\n",
      "L\\equiv p(\\{ x_i \\}|M(\\theta)) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma}\\right)\n",
      "\\end{equation}\n",
      "\n",
      "and we can find the natural log of the likelyhood:\n",
      "\n",
      "\\begin{equation}\n",
      "\\ln(L)\\equiv c-\\sum_{i=1}^N\\frac{-(x_i-\\mu)^2}{2\\sigma}\n",
      "\\end{equation}\n",
      "\n",
      "notice this can be written in terms of the $\\chi^2$:\n",
      "\n",
      "\\begin{equation}\n",
      "\\ln(L)\\equiv c - \\sum_{i=1}^N\\frac{1}{2}z_i = c - \\frac{1}{2}\\chi^2 \n",
      "\\end{equation}\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.3.1 Goodness of fit for a model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The example they give in the book is measuring the luminosity of a star that is intrinisically constant.  Let's start by creating the data set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#first , let's generate the \"data\"\n",
      "np.random.seed(1) #pass the random number generator a seed.\n",
      "\n",
      "N  = 50 #get 50 data points\n",
      "L0 = 10 #the intrinsic luminosity is 10 (data units)\n",
      "dL = 0.2 #the error on each measurement is 0.2 (data units).\n",
      "\n",
      "t = np.linspace(0, 1, N) #take each measurment over a time period [0,1] (time units)\n",
      "L_obs = np.random.normal(L0, dL, N) #get data with a normal distribution around L0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, the true distribution the data was drawn from was a normal distribution with $\\mu=10$ and $\\sigma=0.2$.  Let's plot the data just to make sure it looks like we expect it to."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now take a look at the \"data\"\n",
      "plt.figure()\n",
      "plt.plot([-0.1, 1.3], [L0, L0], ':k', lw=1) #true mean\n",
      "plt.errorbar(t, L_obs, dL, fmt='.k', ecolor='gray', lw=1) #data\n",
      "plt.xlim(-0.05, 1.05)\n",
      "plt.ylim(8.6, 11.4)\n",
      "plt.xlabel('time')\n",
      "plt.ylabel('luminosity')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are now going to pick a model, calculate the maximum likelyhood paramters of the model applied to the data, and calculate the goodness of fit for the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the mean and the chi^2/dof\n",
      "mu = np.mean(L_obs) #by using the mean of the data, we are picking out the maximum likelihood value!\n",
      "z = (L_obs - mu) / dL\n",
      "chi2 = np.sum(z ** 2)\n",
      "chi2dof = chi2 / (N - 1) #this defines the chi^2 per degree of freedom.\n",
      "\n",
      "print \"mean =\", mu\n",
      "print \"chi^2 =\",chi2dof\n",
      "print \"std = \", np.std(L_obs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So this is good.  We got a $\\chi^2$ that is close to 1, and the maximum likelihood value of the mean is pretty darn close to the intrinisic value.  Now, lets take a look at a model that is wrong for comparison."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the mean and the chi^2/dof\n",
      "L_obs_2 = L_obs + 0.5 - t ** 2 #change the data so it is not constant with time\n",
      "mu = np.mean(L_obs_2) #again, this is the MLE for the mean of our model\n",
      "z = (L_obs_2 - mu) / dL\n",
      "chi2 = np.sum(z ** 2)\n",
      "chi2dof = chi2 / (N - 1) #this defines the chi^2 per degree of freedom.\n",
      "\n",
      "print \"mean =\", mu\n",
      "print \"chi^2 =\",chi2dof\n",
      "\n",
      "#now take a look at the \"data\" and model\n",
      "plt.figure()\n",
      "plt.plot([-0.1, 1.3], [L0, L0], ':k', lw=1) #plot the intirinsic value\n",
      "plt.errorbar(t, L_obs_2, dL, fmt='.k', ecolor='gray', lw=1) #plot the \"data\"\n",
      "plt.plot([-0.1, 1.3], [mu, mu], lw=1, color='red') #plot the intirinsic value\n",
      "plt.xlim(-0.05, 1.05)\n",
      "plt.ylim(8.6, 11.4)\n",
      "plt.xlabel('time')\n",
      "plt.ylabel('luminosity')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is important to note that the $\\chi^2$ can only be used to characterize how well a model with a MLE fits the data when the likelihood is gaussian."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.3.2 Model Comparison"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The basic idea here is that we can calculate the maximum likelihood for a set of models.  The model with the largest value **best describes** the data.  However, the model that best describes the data is not necessarly the **best** model.  To account for this we need a scoring system for models.  One example is the Aikake information criterion(AIC).\n",
      "\n",
      "\\begin{equation}\n",
      "{\\rm AIC} \\equiv -2 \\ln(L^0(M))+2k+\\frac{2k(k+1)}{N-k-1}\n",
      "\\end{equation}\n",
      "\n",
      "where $k$ is the number of free model parameters and $N$ is the number of data points.  The first term rewards models for how well they fit the data, the second term penalizes models with too many paramters compared to the number of data points."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.4: Gaussian mixtures: expectation maximization algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The previous example was chosen to be very simple.  We could esaily calculate the MLE of the paramters.  However, it is certainly not the case that we can always find an analytic solution to calculate the MLE.  In that case, we must resort to numerical techniques.\n",
      "\n",
      "A guassian misxture model is just a model that consists of multiple gaussians.  The likelyhood of such a model can be written as the sums of the probabilities of single guassians renormalized.\n",
      "\n",
      "\\begin{equation}\n",
      "p(x_i|\\theta)=\\sum_{j=1}^M\\alpha_j\\mathcal{N}(\\mu_j,\\sigma_j)\n",
      "\\end{equation}\n",
      "\n",
      "where normalization constant must satisfy:\n",
      "\n",
      "\\begin{equation}\n",
      "\\sum_{j=1}^M\\alpha_j=1\n",
      "\\end{equation}\n",
      "\n",
      "From this you can calculate the likelihood function.\n",
      "\n",
      "\\begin{equation}\n",
      "\\ln(L)=\\sum_{i=1}^N\\ln\\left[ \\sum_{j=1}^{M}\\alpha_j\\mathcal{N}(\\mu_j,\\sigma_j)\\right]\n",
      "\\end{equation}\n",
      "\n",
      "Finding the maximum of this function for the $k=(3M-1)$ parameters would not help much as then we would have a system of many equations.  Numerically, it becomes very hard very quckly."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.4.2: Hidden variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "if we have some data that was generated by multiple gaussian functions, we can consider each individual datum as having been generated by only a single gaussian.  We can assign a \"class\" label,$j$, to each datum.  You can see that if we knew the class label for each datum, the problem would be trivial and equivalent to the method above for a single gaussian.  \n",
      "\n",
      "However, since we do not know the the class label for each datum, we can only assign porbabilities to each datum of belonging to a class.  This can be done using Baye's rule:\n",
      "\n",
      "\\begin{equation}\n",
      "p(j|x_i)=\\frac{\\alpha_j\\mathcal{N}(\\mu_j,\\sigma_j)}{\\sum_{j-1}^{M}\\mathcal{N}(\\mu_j,\\sigma_j)}\n",
      "\\end{equation}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.4.3: Expectation maximization algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The key to this technique is to assume that at each iteration, we fix the class assignment probabilities.  We will get to this in a second.  Whatwe really want to do is find the maximum of the likelyhood distribution, so we can take the derivative of the log likelyhood to get:\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial\\ln(L)}{\\partial \\theta_j} = -\\sum_{i=1}^{N}w_{ij}\\frac{\\partial}{\\partial \\theta}\\left( \\ln(\\sigma_j)+\\frac{(x_i-\\mu_j)^2}{2\\sigma_j^2} \\right)\n",
      "\\end{equation}\n",
      "\n",
      "By setting the derivatives with respect to the paramters to zero, you can get the MLE for $\\mu_j$, $\\sigma^2_j$ and $\\alpha_j$.  Then you repeat with the new MLEs until you converge on the answer.\n",
      "\n",
      "Lets give this algorithm a shot.  Below, I am going to create a datset made out of sampleing 2 gaussians.  Them we will run a built in module to solve for the best fit 2 component model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.mixture import GMM\n",
      "\n",
      "mean_1=0\n",
      "mean_2=3\n",
      "X=np.hstack((np.random.normal(mean_1,size=(1000,)),np.random.normal(mean_2,size=(1000,)))) #data\n",
      "model=GMM(2,n_iter=100) #model with 2 gaussians and iterate 1000 times.\n",
      "model.fit(X) #run algorithm to fit.\n",
      "print model.means_\n",
      "print 'model means:',model.means_[0][0],model.means_[1][0] #print result of fit\n",
      "\n",
      "width=0.5\n",
      "bins=np.arange(-10,10,width)\n",
      "result_data = np.histogram(X,bins=bins)[0]\n",
      "result_data = result_data/float(len(X))\n",
      "\n",
      "X_model = model.sample(1000000) #lots of samples!\n",
      "result_model = np.histogram(X_model,bins=bins)[0]\n",
      "result_model = result_model/float(len(X_model))\n",
      "\n",
      "plt.figure()\n",
      "plt.bar(bins[:-1],result_data,width=width) #data\n",
      "plt.plot(bins[:-1]+0.5*(bins[1]-bins[0]),result_model, color='red',lw=5) #model\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('f')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take a look at using different numbers of components and choose the best fit model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = np.arange(1, 11) #number of components \n",
      "models = [None for i in range(len(N))]\n",
      "\n",
      "for i in range(len(N)):\n",
      "    models[i] = GMM(N[i]).fit(X)\n",
      "    \n",
      "AIC = [m.aic(X) for m in models]\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(N,AIC,'o')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "you can see above that the AIC is at a minimum at N=2!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Confidence Estimates: Bootstrap and Jackknife"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "whoops!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}