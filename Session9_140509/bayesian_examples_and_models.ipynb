{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bayesian Examples"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parameter Estimation for a Gaussian (small N)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Section 5.6.1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the case of determining the parameters of a Gaussian ($\\mu$ and $\\sigma$) from a series of $N$ measurements {$x_i$}. There exist simple analytic estimators from classical statistics. Unfortunately, as is often the case, the classical _error bars_/_uncertainties_ on these parameters make assumptions about normality. In the case of a small number of data points $N$, the posterior pdf is _not_ normal."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The classical result is easy, we use the sample standard deviation ($s$) and the sample mean, $\\bar{x}$ as our estimators and we have\n",
      "\n",
      "$\\sigma_x = \\frac{s}{\\sqrt{N}}$ <- Standard error of the mean\n",
      "\n",
      "and\n",
      "\n",
      "$\\sigma_s = \\frac{s}{\\sqrt{2(N-1)}} = \\frac{1}{\\sqrt{2}}\\sqrt{\\frac{N}{N-1}}\\sigma_x$\n",
      "\n",
      "where the extra factors in the latter expression are necessary to make these unbiased estimators (basically because we are estimating the mean and the width of the distribution from the data)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our likelihood is \"simply\"\n",
      "\n",
      "$p({x_i}|\\mu,\\sigma,I) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2 \\sigma^2}\\right)$\n",
      "\n",
      "We will adopt a uniform prior on $\\mu$ and a uniform prior on ln $\\sigma$\n",
      "\n",
      "$p(\\mu,\\sigma|I) \\propto 1/\\sigma$\n",
      "\n",
      "Normally we neglect constants, but they are important in model comparison (later), so we'll keep them here. Our posterior pdf is then\n",
      "\n",
      "$p({x_i}|\\mu,\\sigma,I)p(\\mu,\\sigma|I) = C \\frac{1}{\\sigma^{N+1}} \\prod_{i=1}^{N} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2 \\sigma^2}\\right)$,\n",
      "\n",
      "where\n",
      "\n",
      "$C = (2 \\pi)^{N/2} (\\mu_{\\mathrm {max}} - \\mu_{\\mathrm {min}})^{-1} \\left[\\ln\\left(\\frac{\\sigma_{\\mathrm {max}}}{\\sigma_{\\mathrm {min}}}\\right)\\right]^{-1} $\n",
      "\n",
      "$\\ln (p) = L_{p} = \\mathrm{constant} - (N+1)\\ln \\sigma - \\sum_{i=1}^{N} \\frac{(x_i - \\mu)^2}{2 \\sigma^2}$\n",
      "\n",
      "Much math later you can get the posterior pdf for $\\sigma$ analytically (see textbook)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Author: Jake VanderPlas\n",
      "# License: BSD\n",
      "#   The figure produced by this code is published in the textbook\n",
      "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
      "#   For more information, see http://astroML.github.com\n",
      "#   To report a bug or issue, use the following forum:\n",
      "#    https://groups.google.com/forum/#!forum/astroml-general\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from astroML.stats import mean_sigma\n",
      "from astroML.resample import bootstrap\n",
      "\n",
      "#----------------------------------------------------------------------\n",
      "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
      "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
      "# result in an error if LaTeX is not installed on your system.  In that case,\n",
      "# you can set usetex to False.\n",
      "from astroML.plotting import setup_text_plots\n",
      "setup_text_plots(fontsize=8, usetex=True)\n",
      "\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Define functions for computations below\n",
      "#  These are expected analytic fits to the posterior distributions\n",
      "def compute_pmu(mu, xbar, V, n):\n",
      "    return (1 + (xbar - mu) ** 2 / V) ** (-0.5 * n)\n",
      "\n",
      "\n",
      "def compute_pmu_alt(mu, xbar, V, n):\n",
      "    return (1 + (xbar - mu) ** 2 / V) ** (-0.5 * (n - 1))\n",
      "\n",
      "\n",
      "def compute_psig(sig, V, n):\n",
      "    return (sig ** -n) * np.exp(-0.5 * n * V / sig ** 2)\n",
      "\n",
      "\n",
      "def compute_psig_alt(sig, V, n):\n",
      "    return (sig ** -(n - 1)) * np.exp(-0.5 * n * V / sig ** 2)\n",
      "\n",
      "\n",
      "def gaussian(x, mu, sigma):\n",
      "    return np.exp(-0.5 * (x - mu) ** 2 / sigma ** 2)\n",
      "\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Draw a random sample from the distribution, and compute\n",
      "# some quantities\n",
      "n = 10 ## CHANGE THIS NUMBER TO INCREASE SAMPLES ##\n",
      "xbar = 1\n",
      "V = 4\n",
      "sigma_x = np.sqrt(V)\n",
      "\n",
      "np.random.seed(10)\n",
      "xi = np.random.normal(xbar, sigma_x, size=n)\n",
      "\n",
      "mu_mean, sig_mean = mean_sigma(xi, ddof=1)\n",
      "\n",
      "# compute the analytically expected spread in measurements\n",
      "mu_std = sig_mean / np.sqrt(n)\n",
      "sig_std = sig_mean / np.sqrt(2 * (n - 1))\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# bootstrap estimates\n",
      "mu_bootstrap, sig_bootstrap = bootstrap(xi, 1E6, mean_sigma,\n",
      "                                        kwargs=dict(ddof=1, axis=1))\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Compute analytic posteriors\n",
      "\n",
      "# distributions for the mean\n",
      "mu = np.linspace(-3, 5, 1000)\n",
      "dmu = mu[1] - mu[0]\n",
      "\n",
      "pmu = compute_pmu(mu, xbar, V, n)\n",
      "pmu /= (dmu * pmu.sum())\n",
      "\n",
      "pmu2 = compute_pmu_alt(mu, xbar, V, n)\n",
      "pmu2 /= (dmu * pmu2.sum())\n",
      "\n",
      "pmu_norm = gaussian(mu, mu_mean, mu_std)\n",
      "pmu_norm /= (dmu * pmu_norm.sum())\n",
      "\n",
      "mu_hist, mu_bins = np.histogram(mu_bootstrap, 20)\n",
      "mu_dbin = np.diff(mu_bins).astype(float)\n",
      "mu_hist = mu_hist / mu_dbin / mu_hist.sum()\n",
      "\n",
      "# distributions for the standard deviation\n",
      "sig = np.linspace(1E-4, 8, 1000)\n",
      "dsig = sig[1] - sig[0]\n",
      "psig = compute_psig(sig, V, n)\n",
      "psig /= (dsig * psig.sum())\n",
      "\n",
      "psig2 = compute_psig_alt(sig, V, n)\n",
      "psig2 /= (dsig * psig2.sum())\n",
      "\n",
      "psig_norm = gaussian(sig, sig_mean, sig_std)\n",
      "psig_norm /= (dsig * psig_norm.sum())\n",
      "\n",
      "sig_hist, sig_bins = np.histogram(sig_bootstrap, 20)\n",
      "sig_dbin = np.diff(sig_bins).astype(float)\n",
      "sig_hist = sig_hist / sig_dbin / sig_hist.sum()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------------------------------\n",
      "# Plot the results\n",
      "fig = plt.figure(figsize=(7, 7))\n",
      "fig.subplots_adjust(wspace=0.35, right=0.95,\n",
      "                    hspace=0.2, top=0.95)\n",
      "\n",
      "# plot posteriors for mu\n",
      "ax1 = plt.subplot(221, yscale='log')\n",
      "ax1.plot(mu, pmu, '-b')\n",
      "ax1.plot(mu, pmu2, ':m')\n",
      "ax1.plot(mu, pmu_norm, '--r')\n",
      "ax1.scatter(mu_bins[1:] - 0.5 * mu_dbin, mu_hist,\n",
      "            color='k', facecolor='none')\n",
      "\n",
      "ax1.set_xlabel(r'$\\mu$')\n",
      "ax1.set_ylabel(r'$p(\\mu|x,I)$')\n",
      "\n",
      "ax2 = plt.subplot(223, sharex=ax1)\n",
      "ax2.plot(mu, pmu.cumsum() * dmu, '-b')\n",
      "ax2.plot(mu, pmu_norm.cumsum() * dmu, '--r')\n",
      "ax2.scatter(mu_bins[1:] - 0.5 * mu_dbin, mu_hist.cumsum() * mu_dbin,\n",
      "            color='k', facecolor='none')\n",
      "ax2.set_xlim(-3, 5)\n",
      "\n",
      "ax2.set_xlabel(r'$\\mu$')\n",
      "ax2.set_ylabel(r'$P(<\\mu|x,I)$')\n",
      "\n",
      "# plot posteriors for sigma\n",
      "ax3 = plt.subplot(222, sharey=ax1)\n",
      "ax3.plot(sig, psig, '-b')\n",
      "ax3.plot(sig, psig2, ':m')\n",
      "ax3.plot(sig, psig_norm, '--r')\n",
      "ax3.scatter(sig_bins[1:] - 0.5 * sig_dbin, sig_hist,\n",
      "            color='k', facecolor='none')\n",
      "ax3.set_ylim(1E-4, 2)\n",
      "\n",
      "ax3.set_xlabel(r'$\\sigma$')\n",
      "ax3.set_ylabel(r'$p(\\sigma|x,I)$')\n",
      "\n",
      "ax4 = plt.subplot(224, sharex=ax3, sharey=ax2)\n",
      "\n",
      "ax4.plot(sig, psig.cumsum() * dsig, '-b',label=\"Bayes (Jeffreys)\")\n",
      "ax4.plot(sig, psig2.cumsum() * dsig, ':m',label=\"Bayes (uniform)\")\n",
      "\n",
      "ax4.plot(sig, psig_norm.cumsum() * dsig, '--r',label=\"Classical\")\n",
      "ax4.scatter(sig_bins[1:] - 0.5 * sig_dbin, sig_hist.cumsum() * sig_dbin,\n",
      "            color='k', facecolor='none',label=\"Bootstrap\")\n",
      "ax4.set_ylim(0, 1.05)\n",
      "ax4.set_xlim(0, 5)\n",
      "\n",
      "ax4.set_xlabel(r'$\\sigma$')\n",
      "ax4.set_ylabel(r'$P(<\\sigma|x,I)$')\n",
      "\n",
      "ax4.legend(loc=\"lower right\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The true answer is $\\mu$ = 1 and $\\sigma$ = 2. The classical approximation is not so good for $\\sigma$. Bootstrap is the same (both, effectively, assume a Gaussian posterior pdf). The true posterior pdf (in blue) is not Gaussian; it is a $\\chi^2$ distribution with N-1 degrees of freedom. Remember, this distribution converges to Gaussian for large N.\n",
      "\n",
      "Our choice of (non-informative) prior does not matter. Choosing the uniform or Jeffreys prior gives basically the same answer.\n",
      "\n",
      "Also note that the typical confidence interval on the classical $\\sigma$ could include negative numbers. This is obviously nonsense, although you see it a lot in astronomy. The Bayesian result avoids negative values of $\\sigma$.\n",
      "\n",
      "If you increase $N$, the answers converge.\n",
      "\n",
      "The point is _not_ the classical statistics can't get the right answer. In fact, Cochran's theorem can tell you that the ratio of the sample variance to the true variance is distributed like $\\chi^2_{n-1}$. The point is that in classical statistics you always have to choose the right tool for the job (the right statistic) carefully. There are generally fewer choices in Bayesian analysis, and just one right way to do it."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parameter Estimation for a Gaussian with non-uniform errors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In astronomy we very often have non-uniform errors on data points due to different S/N for different objects. Determining the _mean_ ($\\mu_o$) of an underlying Gaussian is analytic in this case, but there is no analytic solution to determing the _width_ ($\\sigma_0$) of the underlying Gaussian distribution. I actually just ran across this very problem in my own research..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have very non-uniform radial velocities for a sample of young stars in a cluster. The object is to determine the mean and standard deviation of the assumed-to-be-Gaussian underlying distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import astroML.stats as stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verrs,vels = np.load('example_data.npy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errorbar(np.arange(len(vels)),vels,verrs,fmt='ko')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is an analytic estimate given in the textbook. For errors $e_i$, we compute the median error, $e_{50}$, the robust estimate of the standard deviation, $\\sigma_G$ and then the width, $\\sigma_0$ is\n",
      "\n",
      "$\\sigma_0 = \\sqrt{\\zeta^2 \\sigma_G^2 - e_{50}^2}$\n",
      "\n",
      "where $\\zeta = {\\mathrm {median}}(\\tilde{\\sigma_i})/{\\mathrm {mean}}(\\tilde{\\sigma_i})$ with\n",
      "\n",
      "$\\tilde{\\sigma_i} = \\sqrt{\\sigma_G^2+e_i^2-e_{50}^2}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_analytic(vels,verrs):\n",
      "    e_50 = np.median(verrs)\n",
      "    print(\"e50 = {:.3f}\".format(e_50))\n",
      "    \n",
      "    med,sigmaG = stats.median_sigmaG(vels)\n",
      "    print(\"sigma_G = {:.3f}\".format(sigmaG))\n",
      "    \n",
      "    sigma_tilde_i = np.sqrt(sigmaG**2+verrs**2-e_50**2)\n",
      "    #print(sigma_tilde_i)\n",
      "    \n",
      "    c = np.median(sigma_tilde_i)/np.mean(sigma_tilde_i)\n",
      "    print(\"xi = {:.3f}\".format(c))\n",
      "    \n",
      "    sigma_0 = np.sqrt(c**2*sigmaG**2-e_50**2)\n",
      "    print(\"sigma_0 = {:.3f}\".format(sigma_0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_analytic(vels,verrs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What happens if we put in a couple of outliers?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vels2 = np.append(vels,[3.,12.])\n",
      "verrs2 = np.append(verrs,[0.5,0.5])\n",
      "errorbar(np.arange(len(vels2)),vels2,verrs2,fmt='ko')\n",
      "do_analytic(vels2,verrs2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_likelihood(vlsrs,verrs,region_name):\n",
      "\n",
      "    from astroML.plotting import setup_text_plots\n",
      "    from astroML.plotting.mcmc import convert_to_stdev\n",
      "\n",
      "    setup_text_plots(fontsize=8, usetex=True)\n",
      "\n",
      "    def gaussgauss_logL(xi, ei, mu, sigma):\n",
      "        \"\"\"Equation 5.63: gaussian likelihood with gaussian errors\"\"\"\n",
      "        ndim = len(np.broadcast(sigma, mu).shape)\n",
      "\n",
      "        xi = xi.reshape(xi.shape + tuple(ndim * [1]))\n",
      "        ei = ei.reshape(ei.shape + tuple(ndim * [1]))\n",
      "\n",
      "\n",
      "        s2_e2 = sigma ** 2 + ei ** 2\n",
      "        return -0.5 * np.sum(np.log(s2_e2) + (xi - mu) ** 2 / s2_e2, -1 - ndim)\n",
      "\n",
      "    #------------------------------------------------------------\n",
      "    # Define the grid and compute logL\n",
      "    N = len(vlsrs)\n",
      "    ei = verrs\n",
      "    xi = vlsrs\n",
      "\n",
      "    sigma = np.linspace(0.3, 1.6, 70)\n",
      "    mu = np.linspace(7.0, 9.5, 70)\n",
      "\n",
      "    sigma_step = (1.6-0.3)/70.\n",
      "    #prior = np.empty((70,70))\n",
      "    #for xx in np.arange(70):\n",
      "    #    for yy in np.arange(70):\n",
      "    #        prior[yy][xx] = 1.*sigma_step/((yy+1)*sigma_step)\n",
      "            \n",
      "    #prior = np.log(np.flipud(prior))\n",
      "    logL = gaussgauss_logL(xi, ei, mu, sigma[:, np.newaxis])#+prior\n",
      "    \n",
      "    logL -= logL.max()\n",
      "    L = np.exp(logL)\n",
      "\n",
      "    p_sigma = L.sum(1)\n",
      "    p_sigma /= (sigma[1] - sigma[0]) * p_sigma.sum()\n",
      "\n",
      "    p_mu = L.sum(0)\n",
      "    p_mu /= (mu[1] - mu[0]) * p_mu.sum()\n",
      "\n",
      "    #------------------------------------------------------------\n",
      "    # plot the results\n",
      "    fig = plt.figure(figsize=(10, 4))\n",
      "    ax1 = fig.add_subplot(121)\n",
      "\n",
      "    #ax1.imshow(logL, origin='lower',\n",
      "    #           extent=(mu[0], mu[-1], sigma[0], sigma[-1]),\n",
      "    #           cmap=plt.cm.binary,\n",
      "    #           aspect='auto')\n",
      "    #plt.colorbar().set_label(r'$Posterior$',size='xx-large')\n",
      "    #ax1.clim(-5, 0)\n",
      "\n",
      "    ax1.contour( mu, sigma, convert_to_stdev(logL),\n",
      "                levels=(0.683, 0.955, 0.997),\n",
      "                colors='k')\n",
      "    ax1.set_ylabel(r'$\\sigma_0$ (Intrinsic Width [km/s])',size='x-large')\n",
      "    ax1.set_xlabel(r'$\\mu$ (Central Velocity [km/s])',size='x-large')\n",
      "\n",
      "    #plt.show()\n",
      "    #plt.savefig(region_name+\"_trim_erroradj_mle.pdf\")\n",
      "\n",
      "    #plt.clf()\n",
      "    # plot the marginalized distribution\n",
      "\n",
      "    a = p_sigma\n",
      "\n",
      "    b = a.cumsum()\n",
      "    norm_const = b[-1]\n",
      "    b /= b[-1]\n",
      "    #print(b)\n",
      "\n",
      "    yoyo = np.argmax(a)\n",
      "\n",
      "    mmean = sigma[yoyo]\n",
      "    from scipy import interpolate\n",
      "    ya = interpolate.interp1d(b,sigma)\n",
      "\n",
      "    mmin = ya([0.025])\n",
      "    mmax = ya([0.975])\n",
      "    ax2 = fig.add_subplot(122)\n",
      "\n",
      "    ax2.plot(sigma, p_sigma, '-k', label='marginalized')\n",
      "\n",
      "    ax2.fill_between(sigma,p_sigma,p_sigma*0,where=(sigma>mmin)&(sigma<mmax),color='gray',alpha=0.8)\n",
      "    ax2.text(0.6,0.8 ,r'$\\sigma_0 =$ '+str(round(mmean,2)),fontsize='large',transform=ax2.transAxes)\n",
      "    ax2.text(0.6,0.75,r\" 95\\% C.I.= [\"+str(round(mmin,2))+\",\"+str(round(mmax,2))+\"]\",fontsize='large',transform=ax2.transAxes)\n",
      "\n",
      "    ax2.set_xlabel(r'$\\sigma_0$ (Intrinsic Width [km/s])',size='x-large')\n",
      "    ax2.set_ylabel(r'$p(\\sigma_0)$', size='x-large')\n",
      "    #plt.savefig(region_name+\"_pdf_sigma_0.pdf\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_likelihood(vels,verrs,\"test\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Outlier resitant analytic formula and full treatment give different answers. Full treatment is probably biased by outliers. We have an independent way to trim out possible outliers (RV-variable stars)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verrs_trim,vels_trim = np.load('example_data_trim.npy')\n",
      "errorbar(np.arange(len(vels_trim)),vels_trim,verrs_trim,fmt='ko')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_analytic(vels_trim,verrs_trim)\n",
      "do_likelihood(vels_trim,verrs_trim,\"test\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the two estimates agree pretty well. The C.I. on the true spread is larger, because we have fewer points. Eventually we'll see a way to remove outliers from the Bayesian analysis..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parameter Estimation for a Cauchy (Lorentzian) Distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember that the Cauchy distribuion has fat tails and so the mean of a sample is not a good estimator for the center of the distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_xs(N=10,alpha=1.,beta=1.):\n",
      "    theta = np.random.rand(N)*np.pi\n",
      "    x = alpha+beta*np.tan(theta)\n",
      "    return(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_pdf(N=10,nsamp=10000,color='black'):\n",
      "    #Generate xs\n",
      "    np.random.seed(1)\n",
      "    beta = 1.\n",
      "    x = make_xs(N=N)\n",
      "    L = np.zeros(nsamp)\n",
      "    all_alphas = np.linspace(-10,10,nsamp)\n",
      "    for i,alpha in enumerate(all_alphas):\n",
      "        L[i] = -1*np.sum(np.log(beta**2+(x-alpha)**2))\n",
      "    Lmax = np.max(L)\n",
      "    L -= Lmax\n",
      "    posterior = np.exp(L)\n",
      "    xlabel(\"Lighthouse position (alpha)\",size='x-large')\n",
      "    ylabel(\"prob(alpha|{data},I)\",size='x-large')\n",
      "    plot(all_alphas,posterior,color=color)\n",
      "    plot(x,np.ones(len(x))*0.8,'ro')\n",
      "    axvline(np.mean(x),ymin=0,ymax=1,color='r')\n",
      "    axvline(np.median(x),ymin=0,ymax=1,color='r',linestyle=':')\n",
      "    xlim(-10,10)\n",
      "    if color=='black':\n",
      "        figtext(0.70,0.85,\"Ntrials = \"+str(N))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=1024)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bayesian Model Selection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}