{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bayesian Examples"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parameter Estimation for a Gaussian (small N)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Section 5.6.1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the case of determining the parameters of a Gaussian ($\\mu$ and $\\sigma$) from a series of $N$ measurements {$x_i$}. There exist simple analytic estimators from classical statistics. Unfortunately, as is often the case, the classical _error bars_/_uncertainties_ on these parameters make assumptions about normality. In the case of a small number of data points $N$, the posterior pdf is _not_ normal."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The classical result is easy, we use the sample standard deviation ($s$) and the sample mean, $\\bar{x}$ as our estimators and we have\n",
      "\n",
      "$\\sigma_x = \\frac{s}{\\sqrt{N}}$ <- Standard error of the mean\n",
      "\n",
      "and\n",
      "\n",
      "$\\sigma_s = \\frac{s}{\\sqrt{2(N-1)}} = \\frac{1}{\\sqrt{2}}\\sqrt{\\frac{N}{N-1}}\\sigma_x$\n",
      "\n",
      "where the extra factors in the latter expression are necessary to make these unbiased estimators (basically because we are estimating the mean and the width of the distribution from the data)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our likelihood is \"simply\"\n",
      "\n",
      "$p({x_i}|\\mu,\\sigma,I) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2 \\sigma^2}\\right)$\n",
      "\n",
      "We will adopt a uniform prior on $\\mu$ and a uniform prior on ln $\\sigma$\n",
      "\n",
      "$p(\\mu,\\sigma|I) \\propto 1/\\sigma$\n",
      "\n",
      "Normally we neglect constants, but they are important in model comparison (later), so we'll keep them here. Our posterior pdf is then\n",
      "\n",
      "$p({x_i}|\\mu,\\sigma,I)p(\\mu,\\sigma|I) = C \\frac{1}{\\sigma^{N+1}} \\prod_{i=1}^{N} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2 \\sigma^2}\\right)$,\n",
      "\n",
      "where\n",
      "\n",
      "$C = (2 \\pi)^{N/2} (\\mu_{\\mathrm {max}} - \\mu_{\\mathrm {min}})^{-1} \\left[\\ln\\left(\\frac{\\sigma_{\\mathrm {max}}}{\\sigma_{\\mathrm {min}}}\\right)\\right]^{-1} $\n",
      "\n",
      "$\\ln (p) = L_{p} = \\mathrm{constant} - (N+1)\\ln \\sigma - \\sum_{i=1}^{N} \\frac{(x_i - \\mu)^2}{2 \\sigma^2}$\n",
      "\n",
      "Much math later you can get the posterior pdf for $\\sigma$ analytically (see textbook)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Author: Jake VanderPlas\n",
      "# License: BSD\n",
      "#   The figure produced by this code is published in the textbook\n",
      "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
      "#   For more information, see http://astroML.github.com\n",
      "#   To report a bug or issue, use the following forum:\n",
      "#    https://groups.google.com/forum/#!forum/astroml-general\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from astroML.stats import mean_sigma\n",
      "from astroML.resample import bootstrap\n",
      "\n",
      "#----------------------------------------------------------------------\n",
      "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
      "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
      "# result in an error if LaTeX is not installed on your system.  In that case,\n",
      "# you can set usetex to False.\n",
      "from astroML.plotting import setup_text_plots\n",
      "setup_text_plots(fontsize=8, usetex=True)\n",
      "\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Define functions for computations below\n",
      "#  These are expected analytic fits to the posterior distributions\n",
      "def compute_pmu(mu, xbar, V, n):\n",
      "    return (1 + (xbar - mu) ** 2 / V) ** (-0.5 * n)\n",
      "\n",
      "\n",
      "def compute_pmu_alt(mu, xbar, V, n):\n",
      "    return (1 + (xbar - mu) ** 2 / V) ** (-0.5 * (n - 1))\n",
      "\n",
      "\n",
      "def compute_psig(sig, V, n):\n",
      "    return (sig ** -n) * np.exp(-0.5 * n * V / sig ** 2)\n",
      "\n",
      "\n",
      "def compute_psig_alt(sig, V, n):\n",
      "    return (sig ** -(n - 1)) * np.exp(-0.5 * n * V / sig ** 2)\n",
      "\n",
      "\n",
      "def gaussian(x, mu, sigma):\n",
      "    return np.exp(-0.5 * (x - mu) ** 2 / sigma ** 2)\n",
      "\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Draw a random sample from the distribution, and compute\n",
      "# some quantities\n",
      "n = 10 ## CHANGE THIS NUMBER TO INCREASE SAMPLES ##\n",
      "xbar = 1\n",
      "V = 4\n",
      "sigma_x = np.sqrt(V)\n",
      "\n",
      "np.random.seed(10)\n",
      "xi = np.random.normal(xbar, sigma_x, size=n)\n",
      "\n",
      "mu_mean, sig_mean = mean_sigma(xi, ddof=1)\n",
      "\n",
      "# compute the analytically expected spread in measurements\n",
      "mu_std = sig_mean / np.sqrt(n)\n",
      "sig_std = sig_mean / np.sqrt(2 * (n - 1))\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# bootstrap estimates\n",
      "mu_bootstrap, sig_bootstrap = bootstrap(xi, 1E6, mean_sigma,\n",
      "                                        kwargs=dict(ddof=1, axis=1))\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Compute analytic posteriors\n",
      "\n",
      "# distributions for the mean\n",
      "mu = np.linspace(-3, 5, 1000)\n",
      "dmu = mu[1] - mu[0]\n",
      "\n",
      "pmu = compute_pmu(mu, xbar, V, n)\n",
      "pmu /= (dmu * pmu.sum())\n",
      "\n",
      "pmu2 = compute_pmu_alt(mu, xbar, V, n)\n",
      "pmu2 /= (dmu * pmu2.sum())\n",
      "\n",
      "pmu_norm = gaussian(mu, mu_mean, mu_std)\n",
      "pmu_norm /= (dmu * pmu_norm.sum())\n",
      "\n",
      "mu_hist, mu_bins = np.histogram(mu_bootstrap, 20)\n",
      "mu_dbin = np.diff(mu_bins).astype(float)\n",
      "mu_hist = mu_hist / mu_dbin / mu_hist.sum()\n",
      "\n",
      "# distributions for the standard deviation\n",
      "sig = np.linspace(1E-4, 8, 1000)\n",
      "dsig = sig[1] - sig[0]\n",
      "psig = compute_psig(sig, V, n)\n",
      "psig /= (dsig * psig.sum())\n",
      "\n",
      "psig2 = compute_psig_alt(sig, V, n)\n",
      "psig2 /= (dsig * psig2.sum())\n",
      "\n",
      "psig_norm = gaussian(sig, sig_mean, sig_std)\n",
      "psig_norm /= (dsig * psig_norm.sum())\n",
      "\n",
      "sig_hist, sig_bins = np.histogram(sig_bootstrap, 20)\n",
      "sig_dbin = np.diff(sig_bins).astype(float)\n",
      "sig_hist = sig_hist / sig_dbin / sig_hist.sum()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------------------------------\n",
      "# Plot the results\n",
      "fig = plt.figure(figsize=(7, 7))\n",
      "fig.subplots_adjust(wspace=0.35, right=0.95,\n",
      "                    hspace=0.2, top=0.95)\n",
      "\n",
      "# plot posteriors for mu\n",
      "ax1 = plt.subplot(221, yscale='log')\n",
      "ax1.plot(mu, pmu, '-b')\n",
      "ax1.plot(mu, pmu2, ':m')\n",
      "ax1.plot(mu, pmu_norm, '--r')\n",
      "ax1.scatter(mu_bins[1:] - 0.5 * mu_dbin, mu_hist,\n",
      "            color='k', facecolor='none')\n",
      "\n",
      "ax1.set_xlabel(r'$\\mu$')\n",
      "ax1.set_ylabel(r'$p(\\mu|x,I)$')\n",
      "\n",
      "ax2 = plt.subplot(223, sharex=ax1)\n",
      "ax2.plot(mu, pmu.cumsum() * dmu, '-b')\n",
      "ax2.plot(mu, pmu_norm.cumsum() * dmu, '--r')\n",
      "ax2.scatter(mu_bins[1:] - 0.5 * mu_dbin, mu_hist.cumsum() * mu_dbin,\n",
      "            color='k', facecolor='none')\n",
      "ax2.set_xlim(-3, 5)\n",
      "\n",
      "ax2.set_xlabel(r'$\\mu$')\n",
      "ax2.set_ylabel(r'$P(<\\mu|x,I)$')\n",
      "\n",
      "# plot posteriors for sigma\n",
      "ax3 = plt.subplot(222, sharey=ax1)\n",
      "ax3.plot(sig, psig, '-b')\n",
      "ax3.plot(sig, psig2, ':m')\n",
      "ax3.plot(sig, psig_norm, '--r')\n",
      "ax3.scatter(sig_bins[1:] - 0.5 * sig_dbin, sig_hist,\n",
      "            color='k', facecolor='none')\n",
      "ax3.set_ylim(1E-4, 2)\n",
      "\n",
      "ax3.set_xlabel(r'$\\sigma$')\n",
      "ax3.set_ylabel(r'$p(\\sigma|x,I)$')\n",
      "\n",
      "ax4 = plt.subplot(224, sharex=ax3, sharey=ax2)\n",
      "\n",
      "ax4.plot(sig, psig.cumsum() * dsig, '-b',label=\"Bayes (Jeffreys)\")\n",
      "ax4.plot(sig, psig2.cumsum() * dsig, ':m',label=\"Bayes (uniform)\")\n",
      "\n",
      "ax4.plot(sig, psig_norm.cumsum() * dsig, '--r',label=\"Classical\")\n",
      "ax4.scatter(sig_bins[1:] - 0.5 * sig_dbin, sig_hist.cumsum() * sig_dbin,\n",
      "            color='k', facecolor='none',label=\"Bootstrap\")\n",
      "ax4.set_ylim(0, 1.05)\n",
      "ax4.set_xlim(0, 5)\n",
      "\n",
      "ax4.set_xlabel(r'$\\sigma$')\n",
      "ax4.set_ylabel(r'$P(<\\sigma|x,I)$')\n",
      "\n",
      "ax4.legend(loc=\"lower right\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The true answer is $\\mu$ = 1 and $\\sigma$ = 2. The classical approximation is not so good for $\\sigma$. Bootstrap is the same (both, effectively, assume a Gaussian posterior pdf). The true posterior pdf (in blue) is not Gaussian; it is a $\\chi^2$ distribution with N-1 degrees of freedom. Remember, this distribution converges to Gaussian for large N.\n",
      "\n",
      "Our choice of (non-informative) prior does not matter. Choosing the uniform or Jeffreys prior gives basically the same answer.\n",
      "\n",
      "Also note that the typical confidence interval on the classical $\\sigma$ could include negative numbers. This is obviously nonsense, although you see it a lot in astronomy. The Bayesian result avoids negative values of $\\sigma$.\n",
      "\n",
      "If you increase $N$, the answers converge.\n",
      "\n",
      "The point is _not_ the classical statistics can't get the right answer. In fact, Cochran's theorem can tell you that the ratio of the sample variance to the true variance is distributed like $\\chi^2_{n-1}$. The point is that in classical statistics you always have to choose the right tool for the job (the right statistic) carefully. There are generally fewer choices in Bayesian analysis, and just one right way to do it."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parameter Estimation for a Gaussian with non-uniform errors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In astronomy we very often have non-uniform errors on data points due to different S/N for different objects. Determining the _mean_ ($\\mu_o$) of an underlying Gaussian is analytic in this case, but there is no analytic solution to determing the _width_ ($\\sigma_0$) of the underlying Gaussian distribution. I actually just ran across this very problem in my own research..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have very non-uniform radial velocities for a sample of young stars in a cluster. The object is to determine the mean and standard deviation of the assumed-to-be-Gaussian underlying distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import astroML.stats as stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verrs,vels = np.load('example_data.npy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errorbar(np.arange(len(vels)),vels,verrs,fmt='ko')\n",
      "ylabel(\"Radial Velocity [km/s]\",size='x-large')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is an analytic estimate given in the textbook. For errors $e_i$, we compute the median error, $e_{50}$, the robust estimate of the standard deviation, $\\sigma_G$ and then the width, $\\sigma_0$ is\n",
      "\n",
      "$\\sigma_0 = \\sqrt{\\zeta^2 \\sigma_G^2 - e_{50}^2}$\n",
      "\n",
      "where $\\zeta = {\\mathrm {median}}(\\tilde{\\sigma_i})/{\\mathrm {mean}}(\\tilde{\\sigma_i})$ with\n",
      "\n",
      "$\\tilde{\\sigma_i} = \\sqrt{\\sigma_G^2+e_i^2-e_{50}^2}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_analytic(vels,verrs):\n",
      "    e_50 = np.median(verrs)\n",
      "    print(\"e50 = {:.3f}\".format(e_50))\n",
      "    \n",
      "    med,sigmaG = stats.median_sigmaG(vels)\n",
      "    print(\"sigma_G = {:.3f}\".format(sigmaG))\n",
      "    \n",
      "    sigma_tilde_i = np.sqrt(sigmaG**2+verrs**2-e_50**2)\n",
      "    #print(sigma_tilde_i)\n",
      "    \n",
      "    c = np.median(sigma_tilde_i)/np.mean(sigma_tilde_i)\n",
      "    print(\"xi = {:.3f}\".format(c))\n",
      "    \n",
      "    sigma_0 = np.sqrt(c**2*sigmaG**2-e_50**2)\n",
      "    print(\"sigma_0 = {:.3f}\".format(sigma_0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_analytic(vels,verrs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What happens if we put in a couple of outliers?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vels2 = np.append(vels,[3.,12.])\n",
      "verrs2 = np.append(verrs,[0.5,0.5])\n",
      "errorbar(np.arange(len(vels2)),vels2,verrs2,fmt='ko')\n",
      "ylabel(\"Radial Velocity [km/s]\",size='x-large')\n",
      "do_analytic(vels2,verrs2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_likelihood(vlsrs,verrs,region_name):\n",
      "\n",
      "    from astroML.plotting import setup_text_plots\n",
      "    from astroML.plotting.mcmc import convert_to_stdev\n",
      "\n",
      "    setup_text_plots(fontsize=8, usetex=True)\n",
      "\n",
      "    def gaussgauss_logL(xi, ei, mu, sigma):\n",
      "        \"\"\"Equation 5.63: gaussian likelihood with gaussian errors\"\"\"\n",
      "        ndim = len(np.broadcast(sigma, mu).shape)\n",
      "\n",
      "        xi = xi.reshape(xi.shape + tuple(ndim * [1]))\n",
      "        ei = ei.reshape(ei.shape + tuple(ndim * [1]))\n",
      "\n",
      "\n",
      "        s2_e2 = sigma ** 2 + ei ** 2\n",
      "        return -0.5 * np.sum(np.log(s2_e2) + (xi - mu) ** 2 / s2_e2, -1 - ndim)\n",
      "\n",
      "    #------------------------------------------------------------\n",
      "    # Define the grid and compute logL\n",
      "    N = len(vlsrs)\n",
      "    ei = verrs\n",
      "    xi = vlsrs\n",
      "\n",
      "    sigma = np.linspace(0.3, 1.6, 70)\n",
      "    mu = np.linspace(7.0, 9.5, 70)\n",
      "\n",
      "    sigma_step = (1.6-0.3)/70.\n",
      "    #prior = np.empty((70,70))\n",
      "    #for xx in np.arange(70):\n",
      "    #    for yy in np.arange(70):\n",
      "    #        prior[yy][xx] = 1.*sigma_step/((yy+1)*sigma_step)\n",
      "            \n",
      "    #prior = np.log(np.flipud(prior))\n",
      "    logL = gaussgauss_logL(xi, ei, mu, sigma[:, np.newaxis])#+prior\n",
      "    \n",
      "    logL -= logL.max()\n",
      "    L = np.exp(logL)\n",
      "\n",
      "    p_sigma = L.sum(1)\n",
      "    p_sigma /= (sigma[1] - sigma[0]) * p_sigma.sum()\n",
      "\n",
      "    p_mu = L.sum(0)\n",
      "    p_mu /= (mu[1] - mu[0]) * p_mu.sum()\n",
      "\n",
      "    #------------------------------------------------------------\n",
      "    # plot the results\n",
      "    fig = plt.figure(figsize=(10, 4))\n",
      "    ax1 = fig.add_subplot(121)\n",
      "\n",
      "    #ax1.imshow(logL, origin='lower',\n",
      "    #           extent=(mu[0], mu[-1], sigma[0], sigma[-1]),\n",
      "    #           cmap=plt.cm.binary,\n",
      "    #           aspect='auto')\n",
      "    #plt.colorbar().set_label(r'$Posterior$',size='xx-large')\n",
      "    #ax1.clim(-5, 0)\n",
      "\n",
      "    ax1.contour( mu, sigma, convert_to_stdev(logL),\n",
      "                levels=(0.683, 0.955, 0.997),\n",
      "                colors='k')\n",
      "    ax1.set_ylabel(r'$\\sigma_0$ (Intrinsic Width [km/s])',size='x-large')\n",
      "    ax1.set_xlabel(r'$\\mu$ (Central Velocity [km/s])',size='x-large')\n",
      "\n",
      "    #plt.show()\n",
      "    #plt.savefig(region_name+\"_trim_erroradj_mle.pdf\")\n",
      "\n",
      "    #plt.clf()\n",
      "    # plot the marginalized distribution\n",
      "\n",
      "    a = p_sigma\n",
      "\n",
      "    b = a.cumsum()\n",
      "    norm_const = b[-1]\n",
      "    b /= b[-1]\n",
      "    #print(b)\n",
      "\n",
      "    yoyo = np.argmax(a)\n",
      "\n",
      "    mmean = sigma[yoyo]\n",
      "    from scipy import interpolate\n",
      "    ya = interpolate.interp1d(b,sigma)\n",
      "\n",
      "    mmin = ya([0.025])\n",
      "    mmax = ya([0.975])\n",
      "    ax2 = fig.add_subplot(122)\n",
      "\n",
      "    ax2.plot(sigma, p_sigma, '-k', label='marginalized')\n",
      "\n",
      "    ax2.fill_between(sigma,p_sigma,p_sigma*0,where=(sigma>mmin)&(sigma<mmax),color='gray',alpha=0.8)\n",
      "    ax2.text(0.6,0.8 ,r'$\\sigma_0 =$ '+str(round(mmean,2)),fontsize='large',transform=ax2.transAxes)\n",
      "    ax2.text(0.6,0.75,r\" 95\\% C.I.= [\"+str(round(mmin,2))+\",\"+str(round(mmax,2))+\"]\",fontsize='large',transform=ax2.transAxes)\n",
      "\n",
      "    ax2.set_xlabel(r'$\\sigma_0$ (Intrinsic Width [km/s])',size='x-large')\n",
      "    ax2.set_ylabel(r'$p(\\sigma_0)$', size='x-large')\n",
      "    #plt.savefig(region_name+\"_pdf_sigma_0.pdf\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_likelihood(vels,verrs,\"test\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Outlier resitant analytic formula and full treatment give different answers. Full treatment is probably biased by outliers. We have an independent way to trim out possible outliers (RV-variable stars)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verrs_trim,vels_trim = np.load('example_data_trim.npy')\n",
      "errorbar(np.arange(len(vels_trim)),vels_trim,verrs_trim,fmt='ko')\n",
      "ylabel(\"Radial Velocity [km/s]\",size='x-large')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_analytic(vels_trim,verrs_trim)\n",
      "do_likelihood(vels_trim,verrs_trim,\"test\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the two estimates agree pretty well. The C.I. on the true spread is larger, because we have fewer points. Eventually we'll see a way to identify/remove outliers during the Bayesian analysis..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parameter Estimation for a Cauchy (Lorentzian) Distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember that the Cauchy distribuion has fat tails and so the mean of a sample is not a good estimator for the center of the distribution. \n",
      "\n",
      "This is the \"lighthouse\" problem discussed in section 5.6.3, but I'm going to use the simpler 1D problem from Sivia (and notation from therein)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_xs(N=10,alpha=1.,beta=1.):\n",
      "    theta = np.random.rand(N)*np.pi\n",
      "    x = alpha+beta*np.tan(theta)\n",
      "    return(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_pdf(N=10,nsamp=10000,color='black'):\n",
      "    #Generate xs\n",
      "    np.random.seed(1)\n",
      "    beta = 1.\n",
      "    x = make_xs(N=N)\n",
      "    L = np.zeros(nsamp)\n",
      "    all_alphas = np.linspace(-10,10,nsamp)\n",
      "    for i,alpha in enumerate(all_alphas):\n",
      "        L[i] = -1*np.sum(np.log(beta**2+(x-alpha)**2))\n",
      "    Lmax = np.max(L)\n",
      "    L -= Lmax\n",
      "    posterior = np.exp(L)\n",
      "    xlabel(\"Lighthouse position (alpha)\",size='x-large')\n",
      "    ylabel(\"prob(alpha|{data},I)\",size='x-large')\n",
      "    plot(all_alphas,posterior,color=color)\n",
      "    plot(x,np.ones(len(x))*0.8,'ro')\n",
      "    axvline(np.mean(x),ymin=0,ymax=1,color='r')\n",
      "    axvline(np.median(x),ymin=0,ymax=1,color='r',linestyle=':')\n",
      "    xlim(-10,10)\n",
      "    if color=='black':\n",
      "        figtext(0.70,0.85,\"Ntrials = \"+str(N))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_pdf(N=1024)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bayesian Model Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The odds ratio is\n",
      "\n",
      "$O_{21} \\equiv \\frac{p(M_2|D,I)}{p(M_1|D,I)}$.\n",
      "\n",
      "How do we evaluate this?\n",
      "\n",
      "\n",
      "$p(M|D,I) = \\frac{p(D|M,I) p(M|I)}{p(D|I)}$\n",
      "\n",
      "The marginal likelihood (sometimes called evidence) is\n",
      "\n",
      "$E(M) \\equiv p(D|M,I) = \\int p(D|M,\\theta,I) p(\\theta|M,I) d\\theta $\n",
      "\n",
      "Typically, $p(D|I)$ is hard to compute, but it cancels out in the odd ratio so we can just do\n",
      "\n",
      "$O_{21} = \\frac{E(M_2) p(M_2 |I)}{E(M_1) p(M_1|I)} = B_{21} \\frac{p(M_2|I)}{p(M_1|I)} $\n",
      "\n",
      "where \n",
      "\n",
      "$B_{21} \\equiv E(M_2)/E(M_1) $\n",
      "\n",
      "In practice, we often just have to assume that $p(M_2|I) = p(M_1|I)$, but if you really have a preference for one model or the other, you can put it in here in a coherent way. The odds ratio tells you the relative probability of different models. $O_{21} = 10$ means model $M_2$ is 10 times more likely to be correct than $M_1$. But what do we do with this? That's up to you..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Author: Jake VanderPlas\n",
      "# License: BSD\n",
      "#   The figure produced by this code is published in the textbook\n",
      "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
      "#   For more information, see http://astroML.github.com\n",
      "#   To report a bug or issue, use the following forum:\n",
      "#    https://groups.google.com/forum/#!forum/astroml-general\n",
      "import numpy as np\n",
      "from scipy import integrate\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "#----------------------------------------------------------------------\n",
      "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
      "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
      "# result in an error if LaTeX is not installed on your system.  In that case,\n",
      "# you can set usetex to False.\n",
      "from astroML.plotting import setup_text_plots\n",
      "setup_text_plots(fontsize=8, usetex=True)\n",
      "\n",
      "\n",
      "@np.vectorize\n",
      "def odds_ratio(n, k, bstar):\n",
      "    \"\"\"Odds ratio between M_2, where the heads probability is unknown,\n",
      "    and M_1, where the heads probability is known to be `bstar`, evaluated\n",
      "    in the case of `k` heads observed in `n` tosses.\n",
      "\n",
      "    Eqn. 5.25 in the text\n",
      "    \"\"\"\n",
      "    factor = 1. / (bstar ** k * (1 - bstar) ** (n - k))\n",
      "    f = lambda b: b ** k * (1 - b) ** (n - k)\n",
      "\n",
      "    return factor * integrate.quad(f, 0, 1)[0]\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Plot the results\n",
      "fig = plt.figure(figsize=(5, 2.5))\n",
      "fig.subplots_adjust(left=0.13, right=0.95, wspace=0.05, bottom=0.15)\n",
      "\n",
      "subplots = [121, 122]\n",
      "n_array = [20, 60]\n",
      "\n",
      "linestyles = ['-k', '--b']\n",
      "bstar_array = [0.5, 0.1]\n",
      "\n",
      "for subplot, n in zip(subplots, n_array):\n",
      "    ax = fig.add_subplot(subplot, yscale='log')\n",
      "    k = np.arange(n + 1)\n",
      "\n",
      "    # plot curves for two values of bstar\n",
      "    for ls, bstar in zip(linestyles, bstar_array):\n",
      "        ax.plot(k, odds_ratio(n, k, bstar), ls,\n",
      "                label=r'$b^* = %.1f$' % bstar)\n",
      "\n",
      "    if subplot == 121:\n",
      "        ax.set_xlim(0, n - 0.01)\n",
      "        ax.set_ylabel(r'$O_{21}$')\n",
      "        ax.legend(loc=2)\n",
      "    else:\n",
      "        ax.set_xlim(0, n)\n",
      "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
      "\n",
      "    ax.set_xlabel('$k$')\n",
      "    ax.set_title('$n = %i$' % n)\n",
      "    ax.set_ylim(8E-2, 1E3)\n",
      "    ax.xaxis.set_major_locator(plt.MultipleLocator(n / 5))\n",
      "    ax.grid()\n",
      "\n",
      "\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a comparison between a model with no free parameters ($b^*$ known) and a model with one free parameter ($b$). It takes a lot of flips (and a biased coin) to strongly prefer the $b^*$ model. If the data suggests that $b^*$ is wrong, the odds ratio gives strong support for the model with free $b$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Hypothesis testing\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "The first paragraph in section 5.4.1 is rather oddly written.\n",
      "\n",
      "Classical testing would be to see if we can reject the null hypothesis that the coin is fair. \n",
      "\n",
      "Bayesian always requires us to have two models to test. \n",
      "\n",
      "In the above example for $k = 16$ and $N = 20$, classical hypothesis testing allows us to reject the null hypothesis that the coin is fair at $\\alpha$ = 0.05. Bayes allows us to say that the model where we don't know $b$ is 10 times more likely than the model that specifies that $b = 0.5$. But there isn't anything particularly special about the alternative model. If $N = 60$ and $k = 6$, then we can say that the model of $b^* = 0.1$ is 10 times more likely than the model that we don't know $b$. We can make an affirmative statement."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From Jeffreys (1939):\n",
      "\n",
      "\"If there is no clearly stated alternative, and the null hypothesis is rejected, we are left without any rule at all, whereas the null hypothesis, though not satisfactory, may show some sort of correpondence with the facts.\" Newton's theory of gravity would _always_ have failed a $P$-test when looking at planetary orbits (thank to Mercury). \"The success of Newton was not that he explained all the variation of the observed positions of the planets, but that he explained most of it.\" and there was no better alternative yet. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Ockham's Razor"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Discussion from Sivia\n",
      "\n",
      "Consider models $A$ and $B$, where $A$ has no free parameters and $B$ has one, $\\lambda$. If the data likelihood is roughly Gaussian and peaked at $\\lambda_0$ with a width of $\\delta\\lambda$ then you can show that"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image('skematic_forodds.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$B_{AB} = \\frac{p(D|A,I)}{p(D|\\lambda_0,B,I}\\times \\frac{\\lambda_{\\mathrm max} - \\lambda_{\\mathrm min}}{\\delta \\lambda \\sqrt{2 \\pi}}$\n",
      "\n",
      "Assuming that our range on the prior $\\lambda$ is larger than the range permitted by the data ($\\delta \\lambda$), this last factor penalizes us for having a free parameter and is Ockham's Razor arising naturally out of (Bayesian) probability theory.\n",
      "\n",
      "What about if two theorists have the same theory, but different ranges on priors ($\\lambda$, $\\mu$)? Then the evidence/data liklihood is the same and now\n",
      "\n",
      "$B_{AB} = \\frac{\\lambda_{\\mathrm max} - \\lambda_{\\mathrm min}}{\\mu_{\\mathrm max} - \\mu_{\\mathrm min}}$\n",
      "\n",
      "Prefer with theory with the narrower prior (assuming, of course, that the best fit value is within that prior).\n",
      "\n",
      "Finally, if we are choosing between a Gaussian ($\\lambda$) and Lorentzian ($\\mu$), and we assume that the priors on the widths have comparably large ranges then\n",
      "\n",
      "$B_{AB} = \\frac{p(D|\\mu_o,A,I)}{p(D|\\lambda_o,B,I}\\frac{\\delta \\mu}{\\delta \\lambda}$\n",
      "\n",
      "With good data, the first term will dominate. If the evidence is roughly equal then the model with the larger error bar on its paramter is favored."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Author: Jake VanderPlas\n",
      "# License: BSD\n",
      "#   The figure produced by this code is published in the textbook\n",
      "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
      "#   For more information, see http://astroML.github.com\n",
      "#   To report a bug or issue, use the following forum:\n",
      "#    https://groups.google.com/forum/#!forum/astroml-general\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from scipy.stats import cauchy, norm\n",
      "from scipy import integrate\n",
      "\n",
      "#----------------------------------------------------------------------\n",
      "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
      "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
      "# result in an error if LaTeX is not installed on your system.  In that case,\n",
      "# you can set usetex to False.\n",
      "from astroML.plotting import setup_text_plots\n",
      "setup_text_plots(fontsize=8, usetex=True)\n",
      "\n",
      "\n",
      "def logL_cauchy(xi, gamma, mu,\n",
      "                mu_min=-10, mu_max=10, sigma_min=0.01, sigma_max=100):\n",
      "    \"\"\"Equation 5.74: cauchy likelihood\"\"\"\n",
      "    xi = np.asarray(xi)\n",
      "    n = xi.size\n",
      "    shape = np.broadcast(gamma, mu).shape\n",
      "\n",
      "    xi = xi.reshape(xi.shape + tuple([1 for s in shape]))\n",
      "\n",
      "    prior_normalization = - (np.log(mu_max - mu_min)\n",
      "                             + np.log(np.log(sigma_max / sigma_min)))\n",
      "\n",
      "    return (prior_normalization\n",
      "            - n * np.log(np.pi)\n",
      "            + (n - 1) * np.log(gamma)\n",
      "            - np.sum(np.log(gamma ** 2 + (xi - mu) ** 2), 0))\n",
      "\n",
      "\n",
      "def logL_gaussian(xi, sigma, mu,\n",
      "                  mu_min=-10, mu_max=10, sigma_min=0.01, sigma_max=100):\n",
      "    \"\"\"Equation 5.57: gaussian likelihood\"\"\"\n",
      "    xi = np.asarray(xi)\n",
      "    n = xi.size\n",
      "    shape = np.broadcast(sigma, mu).shape\n",
      "\n",
      "    xi = xi.reshape(xi.shape + tuple([1 for s in shape]))\n",
      "\n",
      "    prior_normalization = - (np.log(mu_max - mu_min)\n",
      "                             + np.log(np.log(sigma_max / sigma_min)))\n",
      "\n",
      "    return (prior_normalization\n",
      "            - 0.5 * n * np.log(2 * np.pi)\n",
      "            - (n + 1) * np.log(sigma)\n",
      "            - np.sum(0.5 * ((xi - mu) / sigma) ** 2, 0))\n",
      "\n",
      "\n",
      "def calculate_odds_ratio(xi, epsrel=1E-8, epsabs=1E-15):\n",
      "    \"\"\"\n",
      "    Compute the odds ratio by perfoming a double integral\n",
      "    over the likelihood space.\n",
      "    \"\"\"\n",
      "    gauss_Ifunc = lambda mu, sigma: np.exp(logL_gaussian(xi, mu, sigma))\n",
      "    cauchy_Ifunc = lambda mu, gamma: np.exp(logL_cauchy(xi, mu, gamma))\n",
      "\n",
      "    I_gauss, err_gauss = integrate.dblquad(gauss_Ifunc, -np.inf, np.inf,\n",
      "                                           lambda x: 0, lambda x: np.inf,\n",
      "                                           epsabs=epsabs, epsrel=epsrel)\n",
      "    I_cauchy, err_cauchy = integrate.dblquad(cauchy_Ifunc, -np.inf, np.inf,\n",
      "                                             lambda x: 0, lambda x: np.inf,\n",
      "                                             epsabs=epsabs, epsrel=epsrel)\n",
      "\n",
      "    if I_gauss == 0:\n",
      "        O_CG = np.inf\n",
      "        err_O_CG = np.inf\n",
      "    else:\n",
      "        O_CG = I_cauchy / I_gauss\n",
      "        err_O_CG = O_CG * np.sqrt((err_gauss / I_gauss) ** 2)\n",
      "\n",
      "    return (I_gauss, err_gauss), (I_cauchy, err_cauchy), (O_CG, err_O_CG)\n",
      "\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Draw points from a Cauchy distribution\n",
      "np.random.seed(44)\n",
      "mu = 0\n",
      "gamma = 2\n",
      "xi = cauchy(mu, gamma).rvs(100)\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# compute the odds ratio for the first 10 points\n",
      "((I_gauss, err_gauss),\n",
      " (I_cauchy, err_cauchy),\n",
      " (O_CG, err_O_CG)) = calculate_odds_ratio(xi[:10])\n",
      "\n",
      "print \"Results for first 10 points:\"\n",
      "print \"  L(M = Cauchy) = %.2e +/- %.2e\" % (I_cauchy, err_cauchy)\n",
      "print \"  L(M = Gauss)  = %.2e +/- %.2e\" % (I_gauss, err_gauss)\n",
      "print \"  O_{CG} = %.3g +/- %.3g\" % (O_CG, err_O_CG)\n",
      "\n",
      "\n",
      "((I_gauss, err_gauss),\n",
      " (I_cauchy, err_cauchy),\n",
      " (O_CG, err_O_CG)) = calculate_odds_ratio(xi[:20])\n",
      "\n",
      "print \"Results for first 20 points:\"\n",
      "print \"  L(M = Cauchy) = %.2e +/- %.2e\" % (I_cauchy, err_cauchy)\n",
      "print \"  L(M = Gauss)  = %.2e +/- %.2e\" % (I_gauss, err_gauss)\n",
      "print \"  O_{CG} = %.3g +/- %.3g\" % (O_CG, err_O_CG)\n",
      "\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# calculate the results as a function of number of points\n",
      "Nrange = np.arange(10, 101, 2)\n",
      "Odds = np.zeros(Nrange.shape)\n",
      "for i, N in enumerate(Nrange):\n",
      "    res = calculate_odds_ratio(xi[:N])\n",
      "    Odds[i] = res[2][0]\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# plot the results\n",
      "fig = plt.figure(figsize=(5, 3.75))\n",
      "fig.subplots_adjust(hspace=0.1)\n",
      "\n",
      "ax1 = fig.add_subplot(211, yscale='log')\n",
      "ax1.plot(Nrange, Odds, '-k')\n",
      "ax1.set_ylabel(r'$O_{CG}$ for $N$ points')\n",
      "ax1.set_xlim(0, 100)\n",
      "ax1.xaxis.set_major_formatter(plt.NullFormatter())\n",
      "ax1.yaxis.set_major_locator(plt.LogLocator(base=10000.0))\n",
      "\n",
      "ax2 = fig.add_subplot(212)\n",
      "ax2.scatter(np.arange(1, len(xi) + 1), xi, lw=0, s=16, c='k')\n",
      "ax2.set_xlim(0, 100)\n",
      "ax2.set_xlabel('Sample Size $N$')\n",
      "ax2.set_ylabel('Sample Value')\n",
      "\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}