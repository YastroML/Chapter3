{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Determining the bias of a coin using Bayesian statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Example from _Data Analysis: A Bayesian Tutorial_ by D.S. Silvia"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We seek to determine the bias factor, $H$, for a coin by observing multiple flips of the coin. We will consider different priors and see how the results are dependent on this choice."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We begin by noting that for a coin, the binomial distribution tells us that if we have $data$ such that we see $R$ heads in $N$ tosses, and we define $H$ to that it is equal to 1 for a coin that always comes up heads (and 0 for a coin that always comes up tails), then\n",
      "\n",
      "$\\mathrm {prob}(\\{data\\}|H,I) \\propto H^R(1-H)^{N-R}$.\n",
      "\n",
      "The constant factor out in front of the binomial distribution depends only on $N$ and $R$, and can therefore be neglected. This is just the (unormalized) likelihood of the data. In Bayesian parlance this is sometimes called the _evidence_.\n",
      "\n",
      "Bayes' theorem now tells us that\n",
      "\n",
      "$\\mathrm {prob}(H|\\{data\\},I) \\propto \\mathrm {prob}(\\{data\\}|H,I) \\times \\mathrm {prob}(H|I)$.\n",
      "\n",
      "As a starting point, we'll assume a _flat_ prior such that\n",
      "\n",
      "$\\mathrm {prob}(H|I) = 1$ for 0 $\\leq H \\leq 1$ and 0 otherwise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.stats import binom"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Simulate some random flips of a (potentially-biased) coin"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def flip(p):\n",
      "    return 1 if np.random.random() < p else 0\n",
      "np.random.seed(1)\n",
      "N=100000\n",
      "bias = 0.5\n",
      "coin_tosses = [flip(bias) for i in xrange(N)]\n",
      "print(coin_tosses[0:10])\n",
      "R = np.sum(coin_tosses) #Number of heads\n",
      "print(R)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can simply evaluate $\\mathrm {prob}(H|\\{data\\},I)$ at many finely sampled points between 0 and 1 and this will give us the posterior pdf."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_pdf(N=10,nsamp=1000,prior=None,bias=0.5,color='black',return95=False):\n",
      "    #Generate N flips\n",
      "    np.random.seed(1)\n",
      "    coin_tosses = [flip(bias) for i in xrange(N)]\n",
      "    R = np.sum(coin_tosses) #Number of heads\n",
      "    evidence = np.zeros(nsamp)\n",
      "    posterior = np.zeros(nsamp)\n",
      "    H = np.linspace(0.0001,0.9999,nsamp)\n",
      "    for i in np.arange(nsamp):\n",
      "        #evidence[i] = H[i]**R*(1-H[i])**(N-R)\n",
      "        #The following commented lines provide a\n",
      "        #properly normalized evidence, preventing\n",
      "        #round-off errors at the expense of speed\n",
      "        rv = binom(N,H[i])\n",
      "        evidence[i] = rv.pmf(R)\n",
      "    posterior = evidence*prior #Bayes' theorm\n",
      "    #Normalize our posterior\n",
      "    normalization = np.sum(posterior)/(nsamp)\n",
      "    norm_posterior = posterior/normalization\n",
      "    if return95:\n",
      "        #find 95% confidence interval. Clunky\n",
      "        from scipy import interpolate\n",
      "        b = norm_posterior.cumsum()\n",
      "        norm_const = b[-1]\n",
      "        b /= b[-1]\n",
      "        ya = interpolate.interp1d(b,H)\n",
      "        mmin = ya([0.025])\n",
      "        mmax = ya([0.975])\n",
      "        ci95 = (mmin,mmax)\n",
      "    xlabel(\"Bias-weighting [H]\",size='x-large')\n",
      "    ylabel(\"prob(H|{data},I)\",size='x-large')\n",
      "    plot(H,norm_posterior,color=color)\n",
      "    if color=='black':\n",
      "        figtext(0.70,0.85,\"Ntrials = \"+str(N))\n",
      "    if return95:\n",
      "        return(ci95)\n",
      "    else:\n",
      "        return(None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see what our posterior looks like for increasingly many flips of a coin with a bias factor $H$ = 0.4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sample at 1000 points in [0,1]\n",
      "nsamp = 10000\n",
      "prior = np.ones(nsamp) #This is our flat prior\n",
      "bias = 0.4\n",
      "N=0\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=1\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=2\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=4\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=8\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=16\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=2048\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As the evidence gets better/stronger, the posterior pdf becomes more peaked and converges on the correct answer. For larger number of samples this calculation starts to run into round-off errors (should probably use log(L) instead of L or at least normalize the pmf) but the point is made. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from astropy.modeling import models\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's compare three different priors. We'll compare the above uniform/flat prior with a fairly strong (and wrong!) prior belief that this coin has a bias $H$ = 0.7, and another prior that says that the coin is probably unfair (perhaps we are in Vegas), but we don't know in which sense. We'll also make the coin more biased (H = 0.1)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_multiple_priors(N=0,return95=False):\n",
      "    nsamp = 10000\n",
      "    \n",
      "    #The coin is actually quite biased\n",
      "    bias = 0.1\n",
      "\n",
      "    #This is our flat prior in black\n",
      "    prior = np.ones(nsamp) \n",
      "    ci95_flat = plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior,color='black',return95=return95)\n",
      "    \n",
      "    #Now do a confident Gaussian prior on the wrong value\n",
      "    sigma=0.05\n",
      "    mean=0.7\n",
      "    g = models.Gaussian1D(amplitude = 1./(sigma*np.sqrt(2*math.pi)),mean=mean,stddev=sigma)\n",
      "    prior_wrong_gaussian = g(np.linspace(0,1,nsamp)) \n",
      "    plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior_wrong_gaussian,color='red')\n",
      "    \n",
      "    #Finally, try a prior that the coin is likely double-sided\n",
      "    xx = np.linspace(0,1,nsamp)\n",
      "    prior_vegas = (xx-0.5)**4+0.001\n",
      "    ci95_vegas = plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior_vegas,color='blue',return95=return95)\n",
      "    \n",
      "    if return95:\n",
      "        print(\"Flat prior 95% CI: [{:.2f},{:.2f}]\".format(float(ci95_flat[0]),float(ci95_flat[1])))\n",
      "        print(\"Vegas prior 95% CI: [{:.2f},{:.2f}]\".format(float(ci95_vegas[0]),float(ci95_vegas[1])))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=256)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=512)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=2048)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Bayesian requirement that one be explicit about one's priors is sometimes seen as a drawback. In this case, though, the \"Vegas\" prior -- I'm not sure that the coin is biased, but I'm in Vegas, so it's probably biased -- can help us reach the right conclusion faster. If we were betting on the outcome of coin flips, how much money would the frequentist lose versus the Bayesian (with the \"Vegas\" prior)? That is, how many flips of the coin do we need to make before the frequentist (flat prior) and the Bayesian can conclude with 95% confidence that the coin is not fair?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=2,return95=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Bayseian could rule out that the coin is fair after two flips! That seems a bit drastic. After more flips, 0.5 is back within the 95% CI."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=4,return95=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By 8 flips, the Bayesian 95% confidence interval is fairly stable and rejects the hypothesis that the coin is fair."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=8,return95=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It takes 11 flips before the flat prior (aka the frequentist) can reject the null hypothesis that the coin is biased. Bayes' rule has saved us some money in this imaginary game! The point is -- if you have additional information you should put it in. If you don't, chose an uninformative prior."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=11,return95=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}