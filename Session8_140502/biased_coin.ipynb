{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Determining the bias of a coin using Bayesian statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Example from _Data Analysis: A Bayesian Tutorial_ by D.S. Silvia"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We seek to determine the bias factor, $H$, for a coin by observing multiple flips of the coin. We will consider different priors and see how the results are dependent on this choice."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We begin by noting that for a coin, the binomial distribution tells us that if we have $data$ such that we see $R$ heads in $N$ tosses, and we define $H$ to that it is equal to 1 for a coin that always comes up heads (and 0 for a coin that always comes up tails), then\n",
      "\n",
      "$\\mathrm {prob}(\\{data\\}|H,I) \\propto H^R(1-H)^{N-R}$.\n",
      "\n",
      "The constant factor out in front of the binomial distribution depends only on $N$ and $R$, and can therefore be neglected. This is just the (unormalized) likelihood of the data. In Bayesian parlance this is sometimes called the _evidence_.\n",
      "\n",
      "Bayes' theorem now tells us that\n",
      "\n",
      "$\\mathrm {prob}(H|\\{data\\},I) \\propto \\mathrm {prob}(\\{data\\}|H,I) \\times \\mathrm {prob}(H|I)$.\n",
      "\n",
      "As a starting point, we'll assume a _flat_ prior such that\n",
      "\n",
      "$\\mathrm {prob}(H|I) = 1$ for 0 $\\leq H \\leq 1$ and 0 otherwise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.stats import binom"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Simulate some random flips of a (potentially-biased) coin"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def flip(p):\n",
      "    return 1 if np.random.random() < p else 0\n",
      "np.random.seed(1)\n",
      "N=100000\n",
      "bias = 0.5\n",
      "coin_tosses = [flip(bias) for i in xrange(N)]\n",
      "print(coin_tosses[0:10])\n",
      "R = np.sum(coin_tosses) #Number of heads\n",
      "print(R)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can simply evaluate $\\mathrm {prob}(H|\\{data\\},I)$ at many finely sampled points between 0 and 1 and this will give us the posterior pdf."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_pdf(N=10,nsamp=1000,prior=None,bias=0.5,color='black'):\n",
      "    #Generate N flips\n",
      "    np.random.seed(1)\n",
      "    coin_tosses = [flip(bias) for i in xrange(N)]\n",
      "    R = np.sum(coin_tosses) #Number of heads\n",
      "    evidence = np.zeros(nsamp)\n",
      "    posterior = np.zeros(nsamp)\n",
      "    H = np.linspace(0.0001,0.9999,nsamp)\n",
      "    for i in np.arange(nsamp):\n",
      "        evidence[i] = H[i]**R*(1-H[i])**(N-R)\n",
      "        #The following commented lines provide a\n",
      "        #properly normalized evidence, preventing\n",
      "        #round-off errors at the expense of speed\n",
      "        #rv = binom(N,H[i])\n",
      "        #evidence[i] = rv.pmf(R)\n",
      "    posterior = evidence*prior #Bayes' theorm\n",
      "    #Normalize our posterior\n",
      "    normalization = np.sum(posterior)/(nsamp)\n",
      "    norm_posterior = posterior/normalization\n",
      "    xlabel(\"Bias-weighting [H]\",size='x-large')\n",
      "    ylabel(\"prob(H|{data},I)\",size='x-large')\n",
      "    plot(H,norm_posterior,color=color)\n",
      "    if color=='black':\n",
      "        figtext(0.70,0.85,\"Ntrials = \"+str(N))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see what our posterior looks like for increasingly many flips of a coin with a bias factor $H$ = 0.4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sample at 1000 points in [0,1]\n",
      "nsamp = 10000\n",
      "prior = np.ones(nsamp) #This is our flat prior\n",
      "bias = 0.4\n",
      "N=0\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=1\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=2\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=4\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=8\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=16\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=1024\n",
      "plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As the evidence gets better/stronger, the posterior pdf becomes more peaked and converges on the correct answer. For larger number of samples this calculation starts to run into round-off errors (should probably use log(L) instead of L or at least normalize the pmf) but the point is made. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from astropy.modeling import models\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's compare three different priors. We'll compare the above uniform/flat prior with a fairly strong (and wrong!) prior belief that this coin has a bias $H$ = 0.7, and another prior that says that the coin is probably unfair (perhaps we are in Vegas), but we don't know in which sense."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_multiple_priors(N=0):\n",
      "    nsamp = 10000\n",
      "    #This is our flat prior in black\n",
      "    prior = np.ones(nsamp) \n",
      "    bias = 0.4\n",
      "    plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior,color='black')\n",
      "    #Now do a Gaussian prior on the wrong value\n",
      "    sigma=0.05\n",
      "    mean=0.7\n",
      "    g = models.Gaussian1D(amplitude = 1./(sigma*np.sqrt(2*math.pi)),mean=mean,stddev=sigma)\n",
      "    prior_wrong_gaussian = g(np.linspace(0,1,nsamp)) \n",
      "    plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior_wrong_gaussian,color='red')\n",
      "    #Finally, try a prior that the coin is likely double-sided\n",
      "    xx = np.linspace(0,1,nsamp)\n",
      "    prior_vegas = xx**2-xx+0.3\n",
      "    plot_pdf(N=N,nsamp=nsamp,bias=bias,prior=prior_vegas,color='blue')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=256)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=512)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "do_multiple_priors(N=1024)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}