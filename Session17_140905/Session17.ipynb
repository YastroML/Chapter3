{
 "metadata": {
  "name": "",
  "signature": "sha256:0f1982cc8ccdeb48a7db813c2561a3540d53aeefee2c6a32386260d709b6a1c7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gaussian processes for regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following Mark Ebden's lecture notes, available here: http://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unlike methods encountered so far, Gaussian processes regression (GPR) is less of a parametric tool. \n",
      "\n",
      "Example at hand:\n",
      "Given six noisy data points at hand, we are interested in making a prediction for a seventh at $x_s=0.2$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prep work\n",
      "from __future__ import print_function, division\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.html.widgets import interact, interactive, fixed\n",
      "from scipy.optimize import minimize\n",
      "\n",
      "%matplotlib inline\n",
      "mpl.rcParams['font.size'] = 16\n",
      "mpl.rcParams['figure.figsize'] = (8, 6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# data\n",
      "x = np.array([-1.5, -1, -.75, -.4, -.25, 0])\n",
      "y = 0.55*np.array([-3, -2, -.6, .4, 1, 1.6])\n",
      "sn = 0.3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x, y, 'ko')\n",
      "plt.errorbar(x, y, yerr=sn, fmt=None, ecolor='k')\n",
      "\n",
      "plt.axvline(0.2, color='r')\n",
      "plt.text(0.05, -0.5, \"$x_s$?\")\n",
      "\n",
      "plt.xlim(-1.7, 0.3)\n",
      "plt.ylim(-2, 1.5)\n",
      "\n",
      "plt.xlabel(\"x\")\n",
      "plt.ylabel(\"y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gaussian processes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Formally, a Gaussian process (GP) generates data located throughout some domain such that any finite subset of the range follows a multivariate Gaussian distribution.\n",
      "\n",
      "Now, **the $n$ observations in an arbitrary data set, $\\vec{y}=\\{y_1, y_2, ... , y_n\\}$, can always be imagined as a single point sampled from some multivariate ($n$-variate) Gaussian distribution**, after enough thought. Hence, working backwards, this data set can be partnered with a GP. Thus GPs are as universal as they are simple.\"\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usual choice for the multivariate Gaussian is a zero mean. Then all of the points are related only through a covariance function.\n",
      "\n",
      "Have to assume something for the covariance function (thus, not completely non-parametric approach). A popular choice is a squared exponential:\n",
      "\n",
      "$k(x,x')=\\sigma_f^2 \\exp \\left[ \\frac{-(x-x')^2}{2l^2} \\right]$\n",
      "\n",
      "Has nice properties: high covariance if $x$ and $x'$ close, so that the function is smooth; and small if they are far away, so that distant points don't \"see\" each other (depends on $l$). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Regression intro"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Can think of data points $y_i$ as samples from an underlying function $f(x)$, measured with Gaussian noise: $y=f(x)+\\mathcal{N}(0,\\sigma_n^2)$. \n",
      "\n",
      "We can include the noise part directly in the covariance function, so we'll be directly predicting $y_s$, instead of $f(x_s)$:\n",
      "\n",
      "$k(x,x') = \\sigma_f^2 \\exp \\left[ \\frac{-(x-x')^2}{2l^2} \\right] + \\sigma_n^2 \\delta(x,x')$\n",
      "\n",
      "To prepare for GP regression, we'll first calculate the covariance function between all the points of interest (observed and the one(s) where we want a prediction):\n",
      "\n",
      "$K = \\begin{pmatrix} k(x_1,x_1) & k(x_1,x_2) & \\cdots & k(x_1,x_n) \\\\\n",
      "  k(x_2,x_1) & k(x_2,x_2) & \\cdots & k(x_2,x_n) \\\\\n",
      "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
      "  k(x_n,x_1) & k(x_n,x_2) & \\cdots & k(x_n,x_n)\n",
      "  \\end{pmatrix}$\n",
      "\n",
      "$K_s = \\begin{pmatrix} k(x_s,x_1) & k(x_s,x_2) & \\cdots & k(x_s,x_n) \\end{pmatrix}$\n",
      "\n",
      "$K_{ss} = k(x_s,x_s)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kdelta(x1,x2):\n",
      "    \"\"\"Return element-wise kronecker delta\"\"\"\n",
      "    \n",
      "    delta = x1 - x2\n",
      "    ind = delta == 0\n",
      "    delta[ind] = 1.\n",
      "    delta[~ind] = 0.\n",
      "    \n",
      "    return delta\n",
      "\n",
      "def cov_sqrtexp(x1, x2, pars=(1,1,0)):\n",
      "    \"\"\"Return covariance matrix for vectors x1, x2, given parameters: \n",
      "    pars = (sigma_f, l, sigma_n) \n",
      "    Default: pars = (1, 1, 0)\"\"\"\n",
      "    \n",
      "    mx1, mx2 = np.meshgrid(x1, x2, indexing='ij')\n",
      "    k = pars[0]**2*np.exp( -(mx1-mx2)**2/(2*pars[1]**2) ) + pars[2]**2*kdelta(mx1, mx2)\n",
      "    \n",
      "    return k"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# choose parameters for covariance function (more later)\n",
      "sf = 1.27\n",
      "l = 0.99\n",
      "pars = (sf, l, sn)\n",
      "xs=0.2\n",
      "\n",
      "K = cov_sqrtexp(x, x, pars)\n",
      "Ks = cov_sqrtexp(xs, x, pars)\n",
      "Kss = cov_sqrtexp(xs, xs, pars)\n",
      "\n",
      "Kinv = np.linalg.inv(K)\n",
      "KsT = Ks.T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"\"\"K = {0} \n",
      "Ks = {1} \n",
      "Kss = {2}\"\"\".format(K, Ks, Kss))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Results from GP regression "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ys = Ks.dot(Kinv).dot(y)\n",
      "es = Kss - Ks.dot(Kinv).dot(KsT)\n",
      "\n",
      "print(\"{0} +- {1}\".format(ys, es))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x, y, 'ko')\n",
      "plt.errorbar(x, y, yerr=sn, fmt=None, ecolor='k')\n",
      "\n",
      "plt.plot(xs, ys, 'ro')\n",
      "plt.errorbar(xs, ys, yerr=1.96*es, fmt=None, ecolor='r') # factor of 1.96 x sigma for 95% confidence interval\n",
      "\n",
      "plt.xlim(-1.7, 0.3)\n",
      "plt.ylim(-2, 1.5)\n",
      "\n",
      "plt.xlabel(\"x\")\n",
      "plt.ylabel(\"y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeating for many points\n",
      "\n",
      "def gpr(xt, yt, xp, pars):\n",
      "    \"\"\"xt, yt - training, xp - prediction, pars\"\"\"\n",
      "    \n",
      "    # calculate covariances\n",
      "    K = cov_sqrtexp(xt, xt, pars)\n",
      "    Ks = cov_sqrtexp(xp, xt, pars)\n",
      "    Kss = cov_sqrtexp(xp, xp, pars)\n",
      "\n",
      "    Kinv = np.linalg.inv(K)\n",
      "    KsT = Ks.T\n",
      "\n",
      "    # mean and sigma\n",
      "    ys = Ks.dot(Kinv).dot(yt)\n",
      "    es = Kss - Ks.dot(Kinv).dot(KsT)\n",
      "    \n",
      "    return (ys, es)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xp = np.linspace(-1.7,0.3,100)\n",
      "yp, ep = gpr(x, y, xp, pars)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "yup = yp + 1.96*np.sqrt(np.diag(ep))\n",
      "ydn = yp - 1.96*np.sqrt(np.diag(ep))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x, y, 'ko')\n",
      "plt.errorbar(x, y, yerr=sn, fmt=None, ecolor='k')\n",
      "\n",
      "plt.plot(xp, yp, 'r-')\n",
      "plt.fill_between(xp, ydn, yup, color='r', alpha=0.3)\n",
      "\n",
      "plt.xlim(-1.7, 0.3)\n",
      "plt.ylim(-2.5, 2)\n",
      "\n",
      "plt.xlabel(\"x\")\n",
      "plt.ylabel(\"y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Same thing using the scikit-learn package\n",
      "from sklearn.gaussian_process import GaussianProcess\n",
      "#X = np . random . random ( ( 1 0 0 , 2 ) ) # 1 0 0 pts in 2 dims\n",
      "#y = np . sin ( 1 0 * X [ : , 0 ] + X [ : , 1 ] )\n",
      "gp = GaussianProcess(corr='squared_exponential')\n",
      "gp.fit(np.atleast_2d(x).T, y)\n",
      "yp2, ep2 = gp.predict(np.atleast_2d(xp).T, eval_MSE=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# upper and lower limits (95% confidence)\n",
      "yup2 = yp2 + 1.96*np.sqrt(ep2)\n",
      "ydn2 = yp2 - 1.96*np.sqrt(ep2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# GPR manual + sklearn\n",
      "plt.plot(x, y, 'ko')\n",
      "plt.errorbar(x, y, yerr=sn, fmt=None, ecolor='k')\n",
      "\n",
      "plt.plot(xp, yp, 'r-')\n",
      "plt.fill_between(xp, ydn, yup, color='r', alpha=0.3)\n",
      "\n",
      "plt.plot(xp, yp2, 'b-')\n",
      "plt.fill_between(xp, ydn2, yup2, color='b', alpha=0.3)\n",
      "\n",
      "plt.xlim(-1.7, 0.3)\n",
      "plt.ylim(-2.5, 2)\n",
      "\n",
      "plt.xlabel(\"x\")\n",
      "plt.ylabel(\"y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# playing with errors\n",
      "yp3, ep3 = gpr(x, y, xp, (sf,l,0.00003))\n",
      "\n",
      "yup3 = yp3 + 1.96*np.sqrt(np.diag(ep3))\n",
      "ydn3 = yp3 - 1.96*np.sqrt(np.diag(ep3))\n",
      "\n",
      "plt.plot(x, y, 'ko')\n",
      "plt.errorbar(x, y, yerr=sn, fmt=None, ecolor='k')\n",
      "\n",
      "plt.plot(xp, yp2, 'b-')\n",
      "plt.fill_between(xp, ydn2, yup2, color='b', alpha=0.3)\n",
      "\n",
      "plt.plot(xp, yp3, 'r-')\n",
      "plt.fill_between(xp, ydn3, yup3, color='r', alpha=0.3)\n",
      "\n",
      "plt.xlim(-1.7, 0.3)\n",
      "plt.ylim(-2.5, 2)\n",
      "\n",
      "plt.xlabel(\"x\")\n",
      "plt.ylabel(\"y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Actually doing the regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Log-likelihood of observed points $y$ being drawn from a model $\\theta$:\n",
      "\n",
      "$\\ln p(y|x,\\theta) = -\\frac{1}{2}y^T K^{-1} y - \\frac{1}{2}\\ln|K| -\\frac{n}{2}\\ln 2\\pi$\n",
      "\n",
      "For now just searching for $\\theta$ that maximizes this likelihood (effectively assuming noninformative, flat priors)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# likelihood\n",
      "def lnp(pars, x, y, s, sign=-1):\n",
      "    \"\"\"Likelihood p(y|x,pars) * sign (default: sign=-1.)\"\"\"\n",
      "    \n",
      "    # include free parameters and observational errors as parameters for covariance function\n",
      "    params = np.append(pars,s)\n",
      "    K = cov_sqrtexp(x, x, params)\n",
      "    Kinv = np.linalg.inv(K)\n",
      "    n = np.size(x)\n",
      "    \n",
      "    lnp = -0.5*y.T.dot(Kinv).dot(y) - 0.5*np.log(np.linalg.det(K)) - 0.5*n*np.log(2*3.14)\n",
      "    \n",
      "    return sign*lnp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res=minimize(lnp, [0.9,0.8], args=(x, y, sn))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"\"\"Maximum likelihood results:\n",
      "sigma_h = {0[0]:.2f}\n",
      "l = {0[1]:.2f}\"\"\".format(res.x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cross-validation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Or how to choose a proper model?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Toy data set:\n",
      "\n",
      "$y_i = x_i \\sin(x_i) +\\epsilon_i$\n",
      "\n",
      "$0<x_i<3$\n",
      "\n",
      "Goal: Find the degree of the polynomial best fitting the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------------------------------\n",
      "# Define our functional form\n",
      "def func(x, dy=0.1):\n",
      "    return np.random.normal(np.sin(x) * x, dy)\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# select the (noisy) data\n",
      "np.random.seed(0)\n",
      "x = np.linspace(0, 3, 22)[1:-1]\n",
      "dy = 0.1\n",
      "y = func(x, dy)\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Select the cross-validation points\n",
      "np.random.seed(1)\n",
      "x_cv = 3 * np.random.random(20)\n",
      "y_cv = func(x_cv)\n",
      "\n",
      "x_fit = np.linspace(0, 3, 1000)\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# First figure: plot points with a linear fit\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "ax.scatter(x, y, marker='x', c='k', s=30)\n",
      "\n",
      "# perform polynomial fit\n",
      "p = np.polyfit(x, y, 1)\n",
      "y_fit = np.polyval(p, x_fit)\n",
      "\n",
      "ax.text(0.03, 0.96, \"d = 1\", transform=plt.gca().transAxes,\n",
      "        ha='left', va='top',\n",
      "        bbox=dict(ec='k', fc='w', pad=10))\n",
      "\n",
      "#ax.plot(x_fit, y_fit, '-b')\n",
      "ax.set_xlabel('$x$')\n",
      "ax.set_ylabel('$y$')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear fit not very good, so let's try increasing the polynomial degree:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_polyfit(x, y, n=1):\n",
      "    \"\"\"Plot data and polynomial fit of degree n (default n=1)\"\"\"\n",
      "    \n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "\n",
      "    ax.scatter(x, y, marker='x', c='k', s=30)\n",
      "\n",
      "    # perform polynomial fit\n",
      "    p = np.polyfit(x, y, n)\n",
      "    y_fit = np.polyval(p, x_fit)\n",
      "\n",
      "    ax.text(0.03, 0.96, \"d = 1\", transform=plt.gca().transAxes,\n",
      "        ha='left', va='top',\n",
      "        bbox=dict(ec='k', fc='w', pad=10))\n",
      "\n",
      "    ax.plot(x_fit, y_fit, '-b')\n",
      "    ax.set_xlabel('$x$')\n",
      "    ax.set_ylabel('$y$')\n",
      "    \n",
      "    plt.xlim(-0.5,3.5)\n",
      "    plt.ylim(0,2)\n",
      "\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interact(plot_polyfit,\n",
      "        x=fixed(x),\n",
      "        y=fixed(y),\n",
      "        n=(1, 20, 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Too low degree leads to **underfitting** and **bias**.\n",
      "\n",
      "Too large degree leads to **overfitting** and high **variance**.\n",
      "\n",
      "Need to quantitatively evaluate both aspects of a model -> use cross-validation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Cross - validating a model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split training data:\n",
      "* training set (50 - 70% of the data)\n",
      "* cross-validation set (half of the rest)\n",
      "* testing set (other half of the rest)\n",
      "\n",
      "Constrain model for a given polynomial degree $d$ using the training set alone. Then calculate errors using:\n",
      "\n",
      "$\\epsilon = \\frac{1}{N} \\Sigma_{i=1}^N (y_i - y_{model}(d, \\theta))$\n",
      "\n",
      "$\\epsilon_{tr}$ - training error\n",
      "\n",
      "$\\epsilon_{cv}$ - cross-validation error\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------------------------------\n",
      "# Define our functional form\n",
      "def func(x, dy=0.1):\n",
      "    return np.random.normal(np.sin(x) * x, dy)\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# select the (noisy) data\n",
      "np.random.seed(0)\n",
      "x = np.linspace(0, 3, 22)[1:-1]\n",
      "dy = 0.1\n",
      "y = func(x, dy)\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Select the cross-validation points\n",
      "np.random.seed(1)\n",
      "x_cv = 3 * np.random.random(20)\n",
      "y_cv = func(x_cv)\n",
      "\n",
      "x_fit = np.linspace(0, 3, 1000)\n",
      "\n",
      "\n",
      "# plot training and cross-validation data\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "ax.scatter(x, y, marker='x', c='k', s=30)\n",
      "ax.scatter(x_cv, y_cv, marker='x', c='r', s=30)\n",
      "\n",
      "# perform polynomial fit\n",
      "p = np.polyfit(x, y, 19)\n",
      "y_fit = np.polyval(p, x_fit)\n",
      "#ax.plot(x_fit, y_fit, '-b')\n",
      "\n",
      "ax.text(0.03, 0.96, \"d = 1\", transform=plt.gca().transAxes,\n",
      "        ha='left', va='top',\n",
      "        bbox=dict(ec='k', fc='w', pad=10))\n",
      "\n",
      "ax.set_xlabel('$x$')\n",
      "ax.set_ylabel('$y$')\n",
      "\n",
      "plt.xlim(-0.5,3.5)\n",
      "plt.ylim(0,2)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------------------------------\n",
      "# Third figure: plot errors as a function of polynomial degree d\n",
      "d = np.arange(0, 21)\n",
      "training_err = np.zeros(d.shape)\n",
      "crossval_err = np.zeros(d.shape)\n",
      "\n",
      "fig = plt.figure(figsize=(8, 10))\n",
      "for i in range(len(d)):\n",
      "    p = np.polyfit(x, y, d[i])\n",
      "    training_err[i] = np.sqrt(np.sum((np.polyval(p, x) - y) ** 2)\n",
      "                              / len(y))\n",
      "    crossval_err[i] = np.sqrt(np.sum((np.polyval(p, x_cv) - y_cv) ** 2)\n",
      "                              / len(y_cv))\n",
      "\n",
      "BIC_train = np.sqrt(len(y)) * training_err / dy + d * np.log(len(y))\n",
      "BIC_crossval = np.sqrt(len(y)) * crossval_err / dy + d * np.log(len(y))\n",
      "\n",
      "ax = fig.add_subplot(211)\n",
      "ax.plot(d, crossval_err, '--k', label='cross-validation')\n",
      "ax.plot(d, training_err, '-k', label='training')\n",
      "ax.plot(d, 0.1 * np.ones(d.shape), ':k')\n",
      "\n",
      "ax.set_xlim(0, 14)\n",
      "ax.set_ylim(0, 0.8)\n",
      "\n",
      "ax.set_xlabel('polynomial degree')\n",
      "ax.set_ylabel('rms error')\n",
      "ax.legend(loc=2)\n",
      "\n",
      "ax = fig.add_subplot(212)\n",
      "ax.plot(d, BIC_crossval, '--k', label='cross-validation')\n",
      "ax.plot(d, BIC_train, '-k', label='training')\n",
      "\n",
      "ax.set_xlim(0, 14)\n",
      "ax.set_ylim(0, 100)\n",
      "\n",
      "ax.legend(loc=2)\n",
      "ax.set_xlabel('polynomial degree')\n",
      "ax.set_ylabel('BIC')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model with smallest cross-validation error and fewest number of parameters to constrain: $d=3$. Also in agreement with BIC ( $\\propto$ - max likelihood + number of free params; see chapter 5.4).\n",
      "\n",
      "Haven't used the test set here; but in general used to check up on the cross-validation results.\n",
      "\n",
      "Variations (for more robust results):\n",
      "* two or more-fold cross validation\n",
      "* cross-validation on different subsamples (random or leaving just 1 data point out)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Learning curves"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Trying to answer a question: Given a model, what kind of data will constrain it better? For example, if we're interested in improving photometric redshifts, does it pay off to increase the training set by getting more spectroscopic redshifts (**increase the training set**) or observe in different filters (**increase the number of features**). \n",
      "\n",
      "**Learning curves** - training and cross-validation errors as a function of training set size."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------------------------------\n",
      "# Fourth figure: plot errors as a function of training set size\n",
      "np.random.seed(0)\n",
      "x = 3 * np.random.random(100)\n",
      "y = func(x)\n",
      "\n",
      "np.random.seed(1)\n",
      "x_cv = 3 * np.random.random(100)\n",
      "y_cv = func(x_cv)\n",
      "\n",
      "Nrange = np.arange(10, 101, 2)\n",
      "\n",
      "fig = plt.figure(figsize=(8,10))\n",
      "fig.subplots_adjust(left=0.15, top=0.95)\n",
      "\n",
      "for subplot, d in zip([211, 212], [2, 3]):\n",
      "    ax = fig.add_subplot(subplot)\n",
      "    training_err = np.zeros(Nrange.shape)\n",
      "    crossval_err = np.zeros(Nrange.shape)\n",
      "\n",
      "    for j, N in enumerate(Nrange):\n",
      "        p = np.polyfit(x[:N], y[:N], d)\n",
      "        training_err[j] = np.sqrt(np.sum((np.polyval(p, x[:N])\n",
      "                                          - y[:N]) ** 2) / len(y))\n",
      "        crossval_err[j] = np.sqrt(np.sum((np.polyval(p, x_cv)\n",
      "                                          - y_cv) ** 2) / len(y_cv))\n",
      "\n",
      "    ax.plot(Nrange, crossval_err, '--k', label='cross-validation')\n",
      "    ax.plot(Nrange, training_err, '-k', label='training')\n",
      "    ax.plot(Nrange, 0.1 * np.ones(Nrange.shape), ':k')\n",
      "    ax.legend(loc=1)\n",
      "    ax.text(0.03, 0.94, \"d = %i\" % d, transform=ax.transAxes,\n",
      "            ha='left', va='top', bbox=dict(ec='k', fc='w', pad=10))\n",
      "\n",
      "    ax.set_ylim(0, 0.4)\n",
      "\n",
      "    ax.set_xlabel('Number of training points')\n",
      "    ax.set_ylabel('rms error')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a given model:\n",
      "1. Training error increases with the number of training points ($N$).\n",
      "2. Cross validation error decreases with $N$.\n",
      "3. Up to the statistical error, $\\epsilon_{cr}\\geq\\epsilon_{tr}$.\n",
      "4. For large $N$, $\\epsilon_{cr}$ and $\\epsilon_{tr}$ converge.\n",
      "\n",
      "When $\\epsilon_{cr}\\simeq\\epsilon_{tr}$, error dominated by bias (underfitting). Either add more features to the data, or increase model complexity.\n",
      "\n",
      "When $\\epsilon_{cr}\\gt\\epsilon_{tr}$, error dominated by variance (overfitting). Larger training set would help, as would decreasing the model complexity."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parting thoughts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*\"Cross-validation is one place where machine learning and data mining may be\n",
      "considered more of an art than a science. The optimal route to improving a model\n",
      "is not always straightforward. We hope that by following the suggestions in this\n",
      "chapter, you can apply this art successfully to your own data.\"*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}