{
 "metadata": {
  "name": "",
  "signature": "sha256:6c0dd55e2acb2f57bb36373a0cb8f14a4c9230cd913872936a165053e9fa5b21"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "7. Dimensionality and Its Reduction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import numpy as np\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy\n",
      "\n",
      "%matplotlib inline\n",
      "mpl.rcParams['figure.figsize']=(10.,8.)\n",
      "mpl.rcParams['font.size']=16"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "7.1 The curse of dimensionality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "hypervolume of a D dimensional sphere"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vd(r,d):\n",
      "    return 2*r**d*math.pi**(d/2)/(d*math.gamma(d/2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "volume fraction of unit D sphere in a two-unit D cube"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r=1\n",
      "d=np.arange(2,60)\n",
      "fd=[vd(r,i)/(2*r)**i for i in d]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "volume fraction decreases exponentially with dimension -- need exponentially more points to uniformly sample the whole space"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(d,fd)\n",
      "plt.xlabel(\"Dimension D\")\n",
      "plt.ylabel(\"Volume fraction of a unit D-sphere\")\n",
      "plt.gca().set_yscale('log')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "7.2 Multidimensional datasets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Rotated 2D Gaussian"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma1 = 0.25\n",
      "sigma2 = 0.08\n",
      "rotation = np.pi / 6\n",
      "s = np.sin(rotation)\n",
      "c = np.cos(rotation)\n",
      "\n",
      "X = np.random.normal(0, [sigma1, sigma2], size=(100, 2)).T\n",
      "R = np.array([[c, -s],\n",
      "              [s, c]])\n",
      "X = np.dot(R, X)\n",
      "\n",
      "X=X.T\n",
      "print \"N points, K dimensions\", np.shape(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot Gaussian\n",
      "plt.plot(X[:,0],X[:,1], 'ko')\n",
      "plt.axvline(0, color='k')\n",
      "plt.axhline(0, color='k')\n",
      "plt.xlabel(\"x\")\n",
      "plt.ylabel(\"y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SDSS spectra"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from astroML.datasets import sdss_corrected_spectra\n",
      "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
      "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
      "lam = sdss_corrected_spectra.compute_wavelengths(data)\n",
      "\n",
      "# select random spectra\n",
      "np.random.seed(5)\n",
      "nrows = 5\n",
      "ncols = 3\n",
      "ind = np.random.randint(spectra.shape[0], size=nrows * ncols)\n",
      "spec_sample = spectra[ind]\n",
      "\n",
      "# Array shapes\n",
      "print \"N points, K dimensions\", np.shape(spec_sample)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot spectra\n",
      "fig = plt.figure()\n",
      "\n",
      "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
      "                    bottom=0.1, top=0.95, hspace=0.05)\n",
      "\n",
      "for i in range(ncols):\n",
      "    for j in range(nrows):\n",
      "        ax = fig.add_subplot(nrows, ncols, ncols * j + 1 + i)\n",
      "        ax.plot(lam, spec_sample[ncols * j + i], '-k', lw=1)\n",
      "\n",
      "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
      "        ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
      "        if j < nrows - 1:\n",
      "            ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
      "        else:\n",
      "            plt.xlabel(r'wavelength $(\\AA)$')\n",
      "\n",
      "        ax.set_xlim(3000, 7999)\n",
      "        ylim = ax.get_ylim()\n",
      "        dy = 0.05 * (ylim[1] - ylim[0])\n",
      "        ax.set_ylim(ylim[0] - dy, ylim[1] + dy)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "7.3 Principal component analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- means finding natural coordinate system for the dataset\n",
      "- in the rotated Gaussian example, we know the solution (rotation matrix used to rotate the Gaussian in the first place)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Transform the coordinate axes\n",
      "coord = np.array([[0,1],[0,-1],[1,0],[-1,0]]).T\n",
      "coord_true = np.dot(R, coord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot Gaussian and natural coordinate system\n",
      "plt.figure(figsize=(8,8))\n",
      "plt.plot(X[:,0],X[:,1], 'ko')\n",
      "plt.axvline(0, color='k')\n",
      "plt.axhline(0, color='k')\n",
      "\n",
      "plt.plot(coord_true[0,:2],coord_true[1,:2],'r-', label='true rotation')\n",
      "plt.plot(coord_true[0,2:],coord_true[1,2:],'r-')\n",
      "plt.xlabel(\"x\")\n",
      "plt.ylabel(\"y\")\n",
      "plt.legend(loc=0,frameon=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- more formally: maximizing variance along the coordinate axes + they are orthogonal\n",
      "- first doing it by minimizing the square of distance between points and the principal component axis\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distance_sq(x, X):\n",
      "    \"\"\"Returns least squares distance of points in X from coordinate system rotated by x (rad)\"\"\"\n",
      "    vnorm = np.array([np.sin(x), -np.cos(x)])\n",
      "    d = np.dot(X, vnorm)\n",
      "    return np.sum(d**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Minimize least squares distance\n",
      "from scipy.optimize import minimize\n",
      "x0 = [0.]\n",
      "res = minimize(distance_sq, x0, args=(X,), options={'gtol': 1e-7})\n",
      "print \"True angle: \", rotation\n",
      "print \"Least squares estimate: \", res.x[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Covariance matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- more formally, we can construct a covariance matrix\n",
      "- then we search for rotation which maximizes covariance\n",
      "\n",
      "- by definition, an element of covariance matrix is:\n",
      "\n",
      "\n",
      "$\\Sigma_{ij}=E[(X_i-\\mu_i)(X_j-\\mu_j)]$\n",
      "\n",
      "\n",
      "- in case of centered data (mean subtracted): $\\mu_i=0$\n",
      "- for our sample X (a NxK matrix), the covariance matrix is then: \n",
      "\n",
      "\n",
      "$C_X=\\frac{1}{N-1}X^TX$\n",
      "\n",
      "\n",
      "- given a rotation R: $Y = X R$\n",
      "- covariance $C_Y = R^T X^T X R = R^T C_X R$\n",
      "- R is aligned with the direction of maximal variance if $C_Y$ is diagonal\n",
      "- in other words, we are looking for eigenvectors of covariance matrix $C_X$, which are the principal component axes of $X$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "\"Intuitive\" derivation of PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- searching for first component of $R$, vector $r_1$ in the direction of maximal variance\n",
      "- ie, $r_1^T C_X r_1$ should give the largest eigenvalue\n",
      "- we also want the vector $r_1$ to be normalized: $r_1^T r_1=1$\n",
      "- so we maximize the cost function:\n",
      "\n",
      "\n",
      "$\\phi(r_1,\\lambda_1) = r_1^T C_X r_1 - \\lambda_1(r_1^T r_1 - 1)$\n",
      "\n",
      "\n",
      "- setting the derivative to 0, we get:\n",
      "\n",
      "\n",
      "$C_X r_1 - \\lambda_1 r_1=0$\n",
      "\n",
      "\n",
      "$(C_X - \\lambda_1 I) r_1=0$\n",
      "\n",
      "\n",
      "- the unknowns here are $\\lambda_1$ and $r_1$\n",
      "- but if this is to have a non trivial solution, matrix $C_X - \\lambda_1 I$ must be singular\n",
      "- a matrix is singular iff its determinant is 0, so we can use that to find $\\lambda_1$\n",
      "- $\\lambda_1$ is also an eigenvalue of $C_X$, so we can write:\n",
      "\n",
      "\n",
      "$\\lambda_1 = r_1^T C_X r_1$\n",
      "\n",
      "\n",
      "- to find $r_1$ now, we only need to solve this system of linear equations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create covariance matrix\n",
      "N, K = np.shape(X)\n",
      "# center data matrix\n",
      "X = X - np.mean(X, axis=0)\n",
      "\n",
      "def cov_mat(X):\n",
      "    \"\"\"Return covariance matrix of centered dataset X\"\"\"\n",
      "    N = np.shape(X)[0]\n",
      "    return 1/(float(N) - 1) * np.dot(X.T, X)\n",
      "\n",
      "cx = cov_mat(X)\n",
      "#print N, K, np.shape(cx)\n",
      "\n",
      "# find first eigenvalue\n",
      "def get_det(x, M):\n",
      "    l = np.ones(K) * x\n",
      "    ldiag = np.diag(l)\n",
      "    mat = M - ldiag\n",
      "    return np.linalg.det(mat)\n",
      "\n",
      "l0 = [1.]\n",
      "l1 = scipy.optimize.fsolve(get_det, l0, args=(cx,))\n",
      "print \"First eigenvalue: \", l1\n",
      "\n",
      "# there is another solution (2nd, and smaller, eigenvalue)\n",
      "l0 = [-1.]\n",
      "l2 = scipy.optimize.fsolve(get_det, l0, args=(cx,))\n",
      "print \"Second eigenvalue: \", l2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now find eigenvectors \n",
      "ml=np.ones(K)*l1\n",
      "r1 = np.linalg.solve(cx, ml)\n",
      "r1=r1/np.sqrt(np.sum(r1**2))\n",
      "print r1, np.sqrt(np.sum(r1**2))\n",
      "print R"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Direct search for covariance matrix eigenvectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evals, evec = np.linalg.eig(cx)\n",
      "print \"First eigenvalue: \", evals[0]\n",
      "print \"Second eigenvalue: \", evals[1]\n",
      "print evec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Correlation matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- finding eigenvalues of a covariance matrix especially useful when $K\\ll N$, because covariance matrix is KxK\n",
      "- sometimes we are in the opposite regime, with a few data points in a high dimensional space ($N \\gg K$)\n",
      "- then it is useful to look at the correlation matrix (again assuming the data has been centered):\n",
      "\n",
      "\n",
      "$M_X=\\frac{1}{N-1}X X^T$\n",
      "\n",
      "- dimension NxN, but has the same eigenvalues and eigenvectors as the covariance matrix\n",
      "- can be faster if $N \\gg K$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create correlation matrix\n",
      "def cor_mat(X):\n",
      "    \"\"\"Return correlation matrix of centered dataset X\"\"\"\n",
      "    N = np.shape(X)[0]\n",
      "    return 1/(float(N) - 1) * np.dot(X, X.T)\n",
      "\n",
      "mx = cor_mat(X)\n",
      "print \"Shape of correlation matrix: \", np.shape(mx)\n",
      "\n",
      "# Find eigenvalues and vectors of a correlation matrix\n",
      "mevals, mevec = np.linalg.eig(cx)\n",
      "print \"First eigenvalue: \", mevals[0]\n",
      "print \"Second eigenvalue: \", mevals[1]\n",
      "print mevec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Timing eigenvalue search with correlation and covariance matrix\n",
      "Z = np.random.random((1000,3))\n",
      "%timeit np.linalg.eig(cov_mat(Z))\n",
      "%timeit np.linalg.eig(cor_mat(Z))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z = np.random.random((3,1000))\n",
      "%timeit np.linalg.eig(cov_mat(Z))\n",
      "%timeit np.linalg.eig(cor_mat(Z))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Singular value decomposition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- any real matrix can be factorized as follows:\n",
      "    \n",
      "\n",
      "$M = U \\Sigma V^T$\n",
      "\n",
      "\n",
      "- $\\Sigma$ is a diagonal matrix with singular values of $M$ on its diagonal\n",
      "- columns of $U$ are called left-singular vectors, and columns of $V$ right-singular vectors\n",
      "- this particular decomposition is related to eigenvalue decomposition such that:\n",
      "    - the left-singular vectors of $M$ are eigenvectors of $M M^T$, or the correlation matrix of $M$\n",
      "    - the right-singular vectors of $M$ are eigenvectors of $M^T M$, or the covariance matrix of $M$\n",
      "    - the non-zero singular values of $M$ (found on the diagonal entries of $\\Sigma$) are the square roots of the non-zero eigenvalues of both $M^T M$ and $M M^T$\n",
      "\n",
      "\n",
      "- in summary, we can get the PCA both with eigenvalue decomposition of covariance or correlation matrix, and with SVD of the data matrix\n",
      "- benefit is not having to construct the covariance/correlation matrix\n",
      "- usually fastest when $K\\sim N$, and a default method for PCA in scikit-learn"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "PCA decomposition of spectra"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- first we find the bases (or coordinate system axes)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import RandomizedPCA\n",
      "from astroML.decorators import pickle_results\n",
      "\n",
      "#----------------------------------------------------------------------\n",
      "# Compute PCA components\n",
      "#  we'll save the results so that they can be re-used\n",
      "@pickle_results('spec_decompositions.pkl')\n",
      "def compute_PCA(n_components=5):\n",
      "    spec_mean = spectra.mean(0)\n",
      "\n",
      "    # PCA: use randomized PCA for speed\n",
      "    pca = RandomizedPCA(n_components - 1)\n",
      "    pca.fit(spectra)\n",
      "    pca_comp = np.vstack([spec_mean,\n",
      "                          pca.components_])\n",
      "    \n",
      "    return pca_comp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_components = 15\n",
      "decompositions = compute_PCA(n_components)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.shape(lam), np.shape(decompositions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#----------------------------------------------------------------------\n",
      "# Plot the results\n",
      "fig = plt.figure()\n",
      "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
      "                    bottom=0.1, top=0.95, hspace=0.05)\n",
      "\n",
      "for i, comp in enumerate(decompositions):\n",
      "    ax = fig.add_subplot(5, 3, 1 + i)\n",
      "    \n",
      "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
      "    ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
      "    if i < 12:\n",
      "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
      "    else:\n",
      "        ax.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
      "\n",
      "    ax.plot(lam, comp, '-k', lw=1)\n",
      "\n",
      "    # plot zero line\n",
      "    xlim = [3000, 7999]\n",
      "    ax.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
      "    ax.set_xlim(xlim)\n",
      "\n",
      "    label = 'component %i' % (i + 1)\n",
      "\n",
      "    ax.text(0.03, 0.94, label, transform=ax.transAxes,\n",
      "                ha='left', va='top')\n",
      "\n",
      "    for l in ax.get_xticklines() + ax.get_yticklines(): \n",
      "        l.set_markersize(2) \n",
      "\n",
      "    # adjust y limits\n",
      "    ylim = plt.ylim()\n",
      "    dy = 0.05 * (ylim[1] - ylim[0])\n",
      "    ax.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reconstruction of a particular spectrum"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#------------------------------------------------------------\n",
      "# Compute PCA components\n",
      "\n",
      "# Eigenvalues can be computed using PCA as in the commented code below:\n",
      "\n",
      "#from sklearn.decomposition import PCA\n",
      "#pca = PCA()\n",
      "#pca.fit(spectra)\n",
      "#evals = pca.explained_variance_ratio_\n",
      "#evals_cs = evals.cumsum()\n",
      "\n",
      "#  because the spectra have been reconstructed from masked values, this\n",
      "#  is not exactly correct in this case: we'll use the values computed\n",
      "#  in the file compute_sdss_pca.py\n",
      "evals = data['evals'] ** 2\n",
      "evals_cs = evals.cumsum()\n",
      "evals_cs /= evals_cs[-1]\n",
      "evecs = data['evecs']\n",
      "spec_mean = spectra.mean(0)\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Find the coefficients of a particular spectrum\n",
      "spec = spectra[1]\n",
      "coeff = np.dot(evecs, spec - spec_mean)\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# Plot the sequence of reconstructions\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "fig.subplots_adjust(hspace=0, top=0.95, bottom=0.1, left=0.12, right=0.93)\n",
      "\n",
      "for i, n in enumerate([0, 4, 8, 20]):\n",
      "    ax = fig.add_subplot(411 + i)\n",
      "    ax.plot(lam, spec, '-', c='gray')\n",
      "    ax.plot(lam, spec_mean + np.dot(coeff[:n], evecs[:n]), '-k')\n",
      "\n",
      "    if i < 3:\n",
      "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
      "\n",
      "    ax.set_ylim(-2, 21)\n",
      "    ax.set_ylabel('flux')\n",
      "\n",
      "    if n == 0:\n",
      "        text = \"mean\"\n",
      "    elif n == 1:\n",
      "        text = \"mean + 1 component\\n\"\n",
      "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
      "    else:\n",
      "        text = \"mean + %i components\\n\" % n\n",
      "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
      "\n",
      "    ax.text(0.02, 0.93, text, ha='left', va='top', transform=ax.transAxes)\n",
      "\n",
      "fig.axes[-1].set_xlabel(r'${\\rm wavelength\\ (\\AA)}$')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- adding up contributions from ~20 components results in a close match to the actual spectrum\n",
      "- starting from 1000 binned values, we reduced the dimensionality 50 times!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Choosing the level of truncation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- or how many components to include in reconstruction\n",
      "- useful to look at the scree plot = eigenvectors ordered by their eigenvalues"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Eigenvalues can be computed using PCA as in the commented code below:\n",
      "\n",
      "#from sklearn.decomposition import PCA\n",
      "#pca = PCA()\n",
      "#pca.fit(spectra)\n",
      "#evals = pca.explained_variance_ratio_\n",
      "#evals_cs = evals.cumsum()\n",
      "\n",
      "#  because the spectra have been reconstructed from masked values, this\n",
      "#  is not exactly correct in this case: we'll use the values computed\n",
      "#  in the file compute_sdss_pca.py\n",
      "evals = data['evals'] ** 2\n",
      "evals_cs = evals.cumsum()\n",
      "evals_cs /= evals_cs[-1]\n",
      "\n",
      "#------------------------------------------------------------\n",
      "# plot the eigenvalues\n",
      "fig = plt.figure()\n",
      "fig.subplots_adjust(hspace=0.05, bottom=0.12)\n",
      "\n",
      "ax = fig.add_subplot(211, xscale='log', yscale='log')\n",
      "ax.grid()\n",
      "ax.plot(evals, c='k')\n",
      "ax.set_ylabel('Normalized Eigenvalues')\n",
      "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
      "ax.set_ylim(5E-4, 100)\n",
      "\n",
      "ax = fig.add_subplot(212, xscale='log')\n",
      "ax.grid()\n",
      "ax.semilogx(evals_cs, color='k')\n",
      "ax.set_xlabel('Eigenvalue Number')\n",
      "ax.set_ylabel('Cumulative Eigenvalues')\n",
      "ax.set_ylim(0.65, 1.00)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- usually targeting number of eigenvectors such that cumulative eigenvalues (or fraction in variance) equals 0.7 - 0.95\n",
      "- if knee is present in the cumulative scree plot, can use it as a truncation limit\n",
      "- or r with average eigenvalue of the covariance matrix (Kaiser's rule), 70% of average eigenvalue (Jolliffe)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "PCA with missing data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ensuring orthogonality"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Large datasets -> can do it iteratively"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}